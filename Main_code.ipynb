{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "889f77e5-96b2-40a3-a86c-6012c810f746",
   "metadata": {},
   "source": [
    "# 1. Active_learning_part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739bd385-36b0-4137-b083-002788989aff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Main_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7737df05-4fb3-41b0-90fc-8bd95f6e52cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import subprocess\n",
    "import shutil\n",
    "import random\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "\n",
    "# ============ Configuration Parameters ============\n",
    "# Base paths\n",
    "BASE_DIR = 'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0'\n",
    "POOL_DATA_DIR = os.path.join(BASE_DIR, 'Pool_data')\n",
    "TOTAL_DATA_DIR = os.path.join(BASE_DIR, 'Total_data') #Test_data\n",
    "WORK_PATH = BASE_DIR\n",
    "\n",
    "# Active Learning Parameters\n",
    "INITIAL_TRAIN_SIZE = 1  # q - Initial training set size\n",
    "ENSEMBLE_SIZE = 4       # m - Number of ensemble models\n",
    "TOP_K = 1               # Number of new points selected per round\n",
    "N_ITERATIONS = 100      # Active learning iterations\n",
    "TRAINING_FRAMES = 500   # N2 - Number of frames for training\n",
    "RANDOM_SEED = 42        # --- NEW ---: Add a global random seed to ensure experimental reproducibility\n",
    "\n",
    "# TCN Model Parameters\n",
    "INPUT_LEN = 50\n",
    "PRED_LEN = 150\n",
    "STRIDE = 10\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 1e-3\n",
    "# --- MODIFICATION ---: Define the range for Dropout rate\n",
    "DROPOUT_BASE = 0.2\n",
    "DROPOUT_RANGE = 0.15 # Each model's dropout will be randomly selected from [0.2-0.15, 0.2+0.15], i.e., [0.05, 0.35]\n",
    "\n",
    "# Test set paths\n",
    "TEST_PATHS = [\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_2atm_1per15',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_3atm_1per17.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_4atm_1per20',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_5atm_1per22.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_6atm_1per25',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_2atm_1per17.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_3atm_1per20',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_4atm_1per22.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_5atm_1per25',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_6atm_1per15',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_2atm_1per20',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_3atm_1per22.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_4atm_1per25',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_5atm_1per15',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_6atm_1per17.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_2atm_1per22.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_3atm_1per25',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_4atm_1per15',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_5atm_1per17.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_6atm_1per20',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_2atm_1per25',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_3atm_1per15',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_4atm_1per17.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_5atm_1per20',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_6atm_1per22.5',\n",
    "]\n",
    "\n",
    "# --- NEW ---: Set global random seed\n",
    "def set_seed(seed):\n",
    "    \"\"\"Sets the random seed for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ============ TCN Model Definition (accepts dropout parameter) ============\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(\n",
    "            nn.Conv1d(in_ch, out_ch, kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation)\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        # --- MODIFICATION ---: dropout is now a configurable parameter\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.conv2 = weight_norm(\n",
    "            nn.Conv1d(out_ch, out_ch, kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation)\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.net = nn.Sequential(self.conv1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        if out.size(2) != res.size(2):\n",
    "            out = out[:, :, :res.size(2)]\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    # --- MODIFICATION ---: Accept dropout parameter\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=3, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation = 2 ** i\n",
    "            in_ch = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_ch = num_channels[i]\n",
    "            padding = (kernel_size-1) * dilation\n",
    "            layers += [TemporalBlock(in_ch, out_ch, kernel_size, stride=1,\n",
    "                                     dilation=dilation, padding=padding,\n",
    "                                     # --- MODIFICATION ---: Pass dropout to TemporalBlock\n",
    "                                     dropout=dropout)]\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class TCNForecast(nn.Module):\n",
    "    # --- MODIFICATION ---: Accept dropout parameter\n",
    "    def __init__(self, input_dim, num_channels, kernel_size, pred_len, dropout=0.2):\n",
    "        super(TCNForecast, self).__init__()\n",
    "        # --- MODIFICATION ---: Pass dropout to TemporalConvNet\n",
    "        self.tcn = TemporalConvNet(input_dim, num_channels, kernel_size, dropout=dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], input_dim * pred_len)\n",
    "        self.input_dim = input_dim\n",
    "        self.pred_len = pred_len\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)  # to (batch, input_dim, seq_len)\n",
    "        y = self.tcn(x)        # (batch, hidden, seq_len)\n",
    "        out = y[:, :, -1]      # last time step\n",
    "        pred = self.linear(out)  # (batch, input_dim*pred_len)\n",
    "        return pred.view(-1, self.pred_len, self.input_dim)\n",
    "\n",
    "# ============ Data Processing Tools ============\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.Y = torch.from_numpy(Y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop\n",
    "\n",
    "# ============ Active Learning Core Functions ============\n",
    "class ActiveLearningFramework:\n",
    "    def __init__(self):\n",
    "        self.pool_info = None\n",
    "        self.train_indices = []\n",
    "        self.test_dirs = TEST_PATHS\n",
    "        self.ensemble_models = []\n",
    "        self.num_species = None\n",
    "        self.iteration = 0\n",
    "        \n",
    "        # Create results directory\n",
    "        self.results_dir = os.path.join(BASE_DIR, f'AL_results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}')\n",
    "        os.makedirs(self.results_dir, exist_ok=True)\n",
    "        \n",
    "        # Load pool information\n",
    "        self.load_pool_info()\n",
    "    \n",
    "    def load_pool_info(self):\n",
    "        \"\"\"Load parameter pool information\"\"\"\n",
    "        pool_info_file = os.path.join(BASE_DIR, 'pool_info.json')\n",
    "        if not os.path.exists(pool_info_file):\n",
    "            raise FileNotFoundError(f\"Pool info file not found: {pool_info_file}\")\n",
    "        \n",
    "        with open(pool_info_file, 'r') as f:\n",
    "            self.pool_info = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded {len(self.pool_info)} parameter point infos\")\n",
    "    \n",
    "    def select_initial_training_set(self):\n",
    "        \"\"\"Step 3: Randomly select initial training set\"\"\"\n",
    "        print(\"\\nStep 3: Selecting initial training set...\")\n",
    "        \n",
    "        # Select only completed points\n",
    "        completed_indices = [\n",
    "            i for i, info in enumerate(self.pool_info)\n",
    "            if info['status'] == 'completed'\n",
    "        ]\n",
    "        \n",
    "        if len(completed_indices) < INITIAL_TRAIN_SIZE:\n",
    "            raise ValueError(f\"Number of completed simulation points ({len(completed_indices)}) is less than initial train size ({INITIAL_TRAIN_SIZE})\")\n",
    "        \n",
    "        # --- MODIFICATION ---: Use the global random seed to select initial points, ensuring reproducibility of this part\n",
    "        # random.seed(88) # Replaced with global seed management\n",
    "        self.train_indices = random.sample(completed_indices, INITIAL_TRAIN_SIZE)\n",
    "        \n",
    "        print(f\"Selected initial training set indices: {self.train_indices}\")\n",
    "        \n",
    "        # Record selected points\n",
    "        train_info = [self.pool_info[i] for i in self.train_indices]\n",
    "        with open(os.path.join(self.results_dir, 'initial_train_set.json'), 'w') as f:\n",
    "            json.dump(train_info, f, indent=2)\n",
    "    \n",
    "    def check_simulation_complete(self, sim_dir):\n",
    "        \"\"\"Check if simulation is complete\"\"\"\n",
    "        log_file = os.path.join(sim_dir, \"log.lammps\")\n",
    "        if not os.path.exists(log_file):\n",
    "            return False\n",
    "        \n",
    "        with open(log_file, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "            if lines:\n",
    "                # Check the last few lines\n",
    "                for line in lines[-5:]:\n",
    "                    if \"Total wall time:\" in line:\n",
    "                        return True\n",
    "        return False\n",
    "    \n",
    "    def run_lammps_simulation(self, sim_dir):\n",
    "        \"\"\"Run LAMMPS simulation (for extended simulation)\"\"\"\n",
    "        in_path = os.path.join(sim_dir, \"in.MoO3S\")\n",
    "        \n",
    "        # Build command\n",
    "        cmd = (\n",
    "            \"module load lammps/20230328-intel-2021.4.0-omp && \"\n",
    "            f\"mpirun -np 48 lmp -in {in_path}\"\n",
    "        )\n",
    "        \n",
    "        print(f\"  Running simulation: {os.path.basename(sim_dir)}\")\n",
    "        \n",
    "        try:\n",
    "            # Run simulation\n",
    "            proc = subprocess.Popen(\n",
    "                cmd,\n",
    "                cwd=sim_dir,\n",
    "                shell=True,\n",
    "                stdout=subprocess.PIPE,\n",
    "                stderr=subprocess.STDOUT,\n",
    "                text=True\n",
    "            )\n",
    "            \n",
    "            # Monitor output\n",
    "            count = 0\n",
    "            done = False\n",
    "            for line in proc.stdout:\n",
    "                count += 1\n",
    "                if count % 100 == 0:\n",
    "                    print(\".\", end=\"\", flush=True)\n",
    "                if \"Total wall time:\" in line:\n",
    "                    print(f\"\\n  Completed: {line.strip()}\")\n",
    "                    done = True\n",
    "                    break\n",
    "            \n",
    "            proc.wait()\n",
    "            \n",
    "            if not done:\n",
    "                print(f\"\\n  Warning: {sim_dir} simulation may not have completed normally\")\n",
    "                return False\n",
    "                \n",
    "            # Run post-processing script\n",
    "            proc_script = os.path.join(WORK_PATH, \"lammps_output_process.py\")\n",
    "            if os.path.exists(proc_script):\n",
    "                ret = subprocess.run(\n",
    "                    [\"python\", proc_script, sim_dir],\n",
    "                    capture_output=True,\n",
    "                    text=True\n",
    "                )\n",
    "                if ret.returncode != 0:\n",
    "                    print(f\"  Post-processing failed: {ret.stderr}\")\n",
    "                    return False\n",
    "                else:\n",
    "                    print(f\"  Post-processing complete\")\n",
    "            else:\n",
    "                print(f\"  Warning: Post-processing script {proc_script} not found\")\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Simulation error: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def run_extended_simulation(self, indices):\n",
    "        \"\"\"Step 4: Run extended simulations for selected points (if needed)\"\"\"\n",
    "        print(\"\\nStep 4: Checking and running extended simulations...\")\n",
    "        \n",
    "        # Initial simulation frames (value set from initialize_pool.py)\n",
    "        INITIAL_FRAMES = 500\n",
    "        \n",
    "        for idx in indices:\n",
    "            info = self.pool_info[idx]\n",
    "            sim_dir = info['sim_dir']\n",
    "            \n",
    "            # Check if extended simulation is needed\n",
    "            if TRAINING_FRAMES > INITIAL_FRAMES:\n",
    "                print(f\"Extended simulation needed: {os.path.basename(sim_dir)} (extending from {INITIAL_FRAMES} to {TRAINING_FRAMES} frames)\")\n",
    "                \n",
    "                # Get parameters\n",
    "                T = info['T']\n",
    "                P = info['P']\n",
    "                ratio = info['ratio']\n",
    "                \n",
    "                # Modify the run steps in the in.MoO3S file\n",
    "                in_path = os.path.join(sim_dir, \"in.MoO3S\")\n",
    "                \n",
    "                # Calculate new total steps\n",
    "                steps_per_frame = 5000\n",
    "                new_run_steps = steps_per_frame * TRAINING_FRAMES\n",
    "                \n",
    "                # Read and modify the file\n",
    "                new_lines = []\n",
    "                with open(in_path, 'r') as f:\n",
    "                    for line in f:\n",
    "                        if line.strip().startswith(\"run\"):\n",
    "                            # Update run steps\n",
    "                            line = f\"run {new_run_steps}\\n\"\n",
    "                        new_lines.append(line)\n",
    "                \n",
    "                # Write back to file\n",
    "                with open(in_path, 'w') as f:\n",
    "                    f.writelines(new_lines)\n",
    "                \n",
    "                print(f\"  Run steps updated to: {new_run_steps}\")\n",
    "                \n",
    "                # Backup old log file (if it exists)\n",
    "                log_file = os.path.join(sim_dir, \"log.lammps\")\n",
    "                if os.path.exists(log_file):\n",
    "                    backup_log = os.path.join(sim_dir, f\"log.lammps.backup_{INITIAL_FRAMES}frames\")\n",
    "                    shutil.move(log_file, backup_log)\n",
    "                    print(f\"  Backed up original log file\")\n",
    "                \n",
    "                # Run extended simulation\n",
    "                success = self.run_lammps_simulation(sim_dir)\n",
    "                \n",
    "                if success:\n",
    "                    print(f\"  Extended simulation completed successfully\")\n",
    "                    info['extended_frames'] = TRAINING_FRAMES\n",
    "                else:\n",
    "                    print(f\"  Extended simulation failed\")\n",
    "                    info['status'] = 'extension_failed'\n",
    "                    \n",
    "            else:\n",
    "                print(f\"No extension needed: {os.path.basename(sim_dir)} (current frames meet requirement)\")\n",
    "    \n",
    "    def find_common_species(self):\n",
    "        \"\"\"Find common species (true subset) across all data\"\"\"\n",
    "        print(\"\\nFinding common species...\")\n",
    "        \n",
    "        # Prepare all paths (training set + test set)\n",
    "        all_paths = []\n",
    "        \n",
    "        # Add training set paths\n",
    "        for idx in self.train_indices:\n",
    "            sim_dir = self.pool_info[idx]['sim_dir']\n",
    "            list_file = os.path.join(sim_dir, 'species_list_initial.txt')\n",
    "            all_paths.append(list_file)\n",
    "        \n",
    "        # Add test set paths\n",
    "        for test_dir in self.test_dirs:\n",
    "            list_file = os.path.join(test_dir, 'species_list_initial.txt')\n",
    "            all_paths.append(list_file)\n",
    "        \n",
    "        # Call found_species_subsets.py script\n",
    "        subset_script = os.path.join(WORK_PATH, 'found_species_subsets.py')\n",
    "        if os.path.exists(subset_script):\n",
    "            cmd = ['python', subset_script] + all_paths\n",
    "            ret = subprocess.run(cmd, capture_output=True, text=True)\n",
    "            if ret.returncode != 0:\n",
    "                print(f\"Warning: True subset finding failed: {ret.stderr}\")\n",
    "            else:\n",
    "                print(\"True subset finding complete\")\n",
    "        else:\n",
    "            print(f\"Warning: found_species_subsets.py script not found\")\n",
    "        \n",
    "        # Get number of species\n",
    "        first_train_dir = self.pool_info[self.train_indices[0]]['sim_dir']\n",
    "        species_list_file = os.path.join(first_train_dir, 'species_list.txt')\n",
    "        \n",
    "        if os.path.exists(species_list_file):\n",
    "            with open(species_list_file, 'r') as f:\n",
    "                species_list = [line.strip() for line in f if line.strip()]\n",
    "                self.num_species = len(species_list)\n",
    "                print(f\"Number of common species: {self.num_species}\")\n",
    "        else:\n",
    "            raise FileNotFoundError(f\"Species list file not found: {species_list_file}\")\n",
    "    \n",
    "    def load_data(self, dir_list, max_frames=TRAINING_FRAMES):\n",
    "        \"\"\"Load data\"\"\"\n",
    "        mats = []\n",
    "        species_lists = []\n",
    "        \n",
    "        for d in dir_list:\n",
    "            mat_path = os.path.join(d, 'species_time_matrix.npy')\n",
    "            list_path = os.path.join(d, 'species_list.txt')\n",
    "            \n",
    "            if not os.path.exists(mat_path) or not os.path.exists(list_path):\n",
    "                # Try using initial files\n",
    "                mat_path = os.path.join(d, 'species_time_matrix_initial.npy')\n",
    "                list_path = os.path.join(d, 'species_list_initial.txt')\n",
    "                \n",
    "                if not os.path.exists(mat_path) or not os.path.exists(list_path):\n",
    "                    print(f\"Warning: Skipping directory {d}, files not found\")\n",
    "                    continue\n",
    "            \n",
    "            mat = np.load(mat_path)\n",
    "            mats.append(mat[:, :max_frames])\n",
    "            \n",
    "            with open(list_path, 'r') as f:\n",
    "                species_lists.append([line.strip() for line in f if line.strip()])\n",
    "        \n",
    "        return mats, species_lists\n",
    "    \n",
    "    def create_windows(self, matrices):\n",
    "        \"\"\"Create sliding window data\"\"\"\n",
    "        X_list, Y_list = [], []\n",
    "        \n",
    "        for mat in matrices:\n",
    "            num_species, T = mat.shape\n",
    "            for start in range(0, T - INPUT_LEN - PRED_LEN + 1, STRIDE):\n",
    "                x = mat[:, start: start + INPUT_LEN]\n",
    "                y = mat[:, start + INPUT_LEN: start + INPUT_LEN + PRED_LEN]\n",
    "                X_list.append(x.T)  # (seq_len, num_species)\n",
    "                Y_list.append(y.T)  # (pred_len, num_species)\n",
    "        \n",
    "        if X_list:\n",
    "            return np.stack(X_list), np.stack(Y_list)\n",
    "        else:\n",
    "            return np.array([]), np.array([])\n",
    "    \n",
    "    def train_ensemble_models(self):\n",
    "        \"\"\"Step 5: Train Deep Ensemble TCN Models (Modified Version)\"\"\"\n",
    "        print(\"\\nStep 5: Training Deep Ensemble TCN Models (Enhanced Diversity)...\")\n",
    "        \n",
    "        # Prepare training and validation data\n",
    "        train_dirs = [self.pool_info[i]['sim_dir'] for i in self.train_indices]\n",
    "        \n",
    "        # Load data\n",
    "        train_mats, _ = self.load_data(train_dirs)\n",
    "        val_mats, _ = self.load_data(self.test_dirs) # Use the fixed test set as the validation set\n",
    "        \n",
    "        # Create window data\n",
    "        X_train_full, Y_train_full = self.create_windows(train_mats)\n",
    "        X_val, Y_val = self.create_windows(val_mats)\n",
    "        \n",
    "        if len(X_train_full) == 0:\n",
    "            raise ValueError(\"Training data is empty\")\n",
    "        \n",
    "        print(f\"Full training data shape: X={X_train_full.shape}, Y={Y_train_full.shape}\")\n",
    "        print(f\"Validation data shape: X={X_val.shape}, Y={Y_val.shape}\")\n",
    "        \n",
    "        # Create validation data loader (shared by all models)\n",
    "        val_loader = DataLoader(\n",
    "            TimeSeriesDataset(X_val, Y_val),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        # Train ensemble models\n",
    "        self.ensemble_models = []\n",
    "        input_dim = X_train_full.shape[2]\n",
    "        hidden_channels = [64, 64, 64]\n",
    "        kernel_size = 3\n",
    "        \n",
    "        n_train_samples = X_train_full.shape[0]\n",
    "\n",
    "        for model_idx in range(ENSEMBLE_SIZE):\n",
    "            print(f\"\\nTraining model {model_idx + 1}/{ENSEMBLE_SIZE}\")\n",
    "\n",
    "            # --- MODIFICATION START: Implement Bagging and Hyperparameter Randomization ---\n",
    "            \n",
    "            # 1. Bagging: Create a bootstrap sampled dataset for the current model\n",
    "            #    By sampling with replacement, create a unique training perspective for each model\n",
    "            bootstrap_indices = np.random.choice(n_train_samples, size=n_train_samples, replace=True)\n",
    "            X_train_boot = X_train_full[bootstrap_indices]\n",
    "            Y_train_boot = Y_train_full[bootstrap_indices]\n",
    "\n",
    "            print(f\"  Using Bagging: Created bootstrap training set of size {len(X_train_boot)}\")\n",
    "\n",
    "            # Create a data loader specific to this model\n",
    "            train_loader = DataLoader(\n",
    "                TimeSeriesDataset(X_train_boot, Y_train_boot),\n",
    "                batch_size=BATCH_SIZE,\n",
    "                shuffle=True # Shuffle the bootstrap set every epoch\n",
    "            )\n",
    "\n",
    "            # 2. Hyperparameter Randomization: Select a random Dropout rate for the model\n",
    "            #    This further increases the diversity among models\n",
    "            dropout_rate = random.uniform(\n",
    "                max(0.0, DROPOUT_BASE - DROPOUT_RANGE),\n",
    "                min(0.5, DROPOUT_BASE + DROPOUT_RANGE)\n",
    "            )\n",
    "            print(f\"  Using random hyperparameter: Dropout rate = {dropout_rate:.4f}\")\n",
    "\n",
    "            # --- MODIFICATION END ---\n",
    "\n",
    "            # Create model (using randomized dropout rate)\n",
    "            model = TCNForecast(input_dim, hidden_channels, kernel_size, PRED_LEN, dropout=dropout_rate)\n",
    "            \n",
    "            # If incremental training and model already exists, load the previous model\n",
    "            # Note: In the AL loop, this usually means continuing training on new data, Bagging is still effective\n",
    "            if self.iteration > 0 and model_idx < len(self.ensemble_models):\n",
    "                old_model = self.ensemble_models[model_idx]\n",
    "                model.load_state_dict(old_model.state_dict())\n",
    "                print(f\"  Loading pre-trained model weights for incremental training\")\n",
    "            \n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "            criterion = nn.MSELoss()\n",
    "            early_stopping = EarlyStopping(patience=10)\n",
    "            \n",
    "            best_val_loss = float('inf')\n",
    "            best_state_dict = None\n",
    "            \n",
    "            # Training loop\n",
    "            for epoch in range(1, EPOCHS + 1):\n",
    "                # Train\n",
    "                model.train()\n",
    "                total_loss = 0.0\n",
    "                for x_batch, y_batch in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = criterion(model(x_batch), y_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                \n",
    "                avg_train = total_loss / len(train_loader)\n",
    "                \n",
    "                # Validate\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for x_batch, y_batch in val_loader:\n",
    "                        val_loss += criterion(model(x_batch), y_batch).item()\n",
    "                \n",
    "                avg_val = val_loss / len(val_loader)\n",
    "                \n",
    "                if epoch % 20 == 0:\n",
    "                    print(f\"  Epoch {epoch}: Train Loss={avg_train:.4f}, Val Loss={avg_val:.4f}\")\n",
    "                \n",
    "                # Save the best model\n",
    "                if avg_val < best_val_loss:\n",
    "                    best_val_loss = avg_val\n",
    "                    best_state_dict = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "                # Early stopping check\n",
    "                if early_stopping(avg_val):\n",
    "                    print(f\"  Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            \n",
    "            # Restore the best model\n",
    "            if best_state_dict is not None:\n",
    "                model.load_state_dict(best_state_dict)\n",
    "            \n",
    "            # Save the model\n",
    "            model_path = os.path.join(\n",
    "                self.results_dir,\n",
    "                f'model_{model_idx}_iter_{self.iteration}.pth'\n",
    "            )\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            self.ensemble_models.append(model)\n",
    "        \n",
    "        print(f\"\\nEnsemble model training complete, total {len(self.ensemble_models)} models\")\n",
    "    \n",
    "    def calculate_uncertainty(self, data_dir):\n",
    "        \"\"\"Calculate uncertainty for a single data point (using rolling prediction)\"\"\"\n",
    "        # Load data\n",
    "        mat_path = os.path.join(data_dir, 'species_time_matrix_initial.npy')\n",
    "        list_path = os.path.join(data_dir, 'species_list_initial.txt')\n",
    "        \n",
    "        if not os.path.exists(mat_path):\n",
    "            return float('inf')\n",
    "        \n",
    "        # Load matrix\n",
    "        mat_initial = np.load(mat_path)\n",
    "        \n",
    "        # Read species list\n",
    "        with open(list_path, 'r') as f:\n",
    "            species_initial = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        # Get common species list (from the first training set directory)\n",
    "        first_train_dir = self.pool_info[self.train_indices[0]]['sim_dir']\n",
    "        common_species_path = os.path.join(first_train_dir, 'species_list.txt')\n",
    "        \n",
    "        if not os.path.exists(common_species_path):\n",
    "            print(f\"Warning: Common species list not found: {common_species_path}\")\n",
    "            return float('inf')\n",
    "        \n",
    "        with open(common_species_path, 'r') as f:\n",
    "            common_species = [line.strip() for line in f if line.strip()]\n",
    "        \n",
    "        # Create alignment matrix, initialize to 0\n",
    "        aligned_mat = np.zeros((len(common_species), mat_initial.shape[1]))\n",
    "        \n",
    "        # Align existing species\n",
    "        for i, sp in enumerate(common_species):\n",
    "            if sp in species_initial:\n",
    "                # If species exists, copy the corresponding data\n",
    "                idx = species_initial.index(sp)\n",
    "                aligned_mat[i, :] = mat_initial[idx, :]\n",
    "            # If species does not exist, the corresponding row remains 0 (already initialized to 0)\n",
    "        \n",
    "        # Check data length\n",
    "        T = aligned_mat.shape[1]\n",
    "        if T < INPUT_LEN:\n",
    "            print(f\"Warning: Data length insufficient ({T} < {INPUT_LEN})\")\n",
    "            return float('inf')\n",
    "        \n",
    "        # Calculate total prediction steps needed\n",
    "        total_pred_steps = TRAINING_FRAMES - INPUT_LEN  # 500 - 50 = 450\n",
    "        \n",
    "        try:\n",
    "            # Use the starting 50 frames as initial input\n",
    "            initial_input = aligned_mat[:, :INPUT_LEN].T  # (INPUT_LEN, num_species)\n",
    "            \n",
    "            # Perform rolling prediction for each model\n",
    "            all_predictions = []\n",
    "            \n",
    "            for model in self.ensemble_models:\n",
    "                model.eval()\n",
    "                \n",
    "                # Initialize prediction sequence\n",
    "                predicted_sequence = []\n",
    "                current_input = initial_input.copy()\n",
    "                \n",
    "                # Rolling prediction\n",
    "                steps_predicted = 0\n",
    "                while steps_predicted < total_pred_steps:\n",
    "                    # Prepare input tensor\n",
    "                    X_tensor = torch.from_numpy(current_input[np.newaxis, :, :]).float()\n",
    "                    \n",
    "                    # Predict next PRED_LEN steps\n",
    "                    with torch.no_grad():\n",
    "                        pred = model(X_tensor).numpy()[0]  # (PRED_LEN, num_species)\n",
    "                    \n",
    "                    # Determine the number of steps to actually use this time\n",
    "                    steps_to_use = min(PRED_LEN, total_pred_steps - steps_predicted)\n",
    "                    predicted_sequence.append(pred[:steps_to_use])\n",
    "                    steps_predicted += steps_to_use\n",
    "                    \n",
    "                    # If further prediction is needed, update the input\n",
    "                    if steps_predicted < total_pred_steps:\n",
    "                        # Simplified version: Build the next input entirely based on past predictions\n",
    "                        # This is a purer auto-regressive prediction, which can better expose model uncertainty\n",
    "                        current_input = np.vstack(predicted_sequence)[-INPUT_LEN:]\n",
    "            \n",
    "                # Concatenate all predictions\n",
    "                full_prediction = np.vstack(predicted_sequence)  # (total_pred_steps, num_species)\n",
    "                all_predictions.append(full_prediction)\n",
    "            \n",
    "            # Calculate variance of all model predictions\n",
    "            all_predictions = np.stack(all_predictions)  # (num_models, total_pred_steps, num_species)\n",
    "            \n",
    "            # Calculate variance for each time step and species, then take the mean\n",
    "            variance = np.var(all_predictions, axis=0)  # (total_pred_steps, num_species)\n",
    "            uncertainty = np.mean(variance)\n",
    "            \n",
    "            return uncertainty\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error during uncertainty calculation: {e}\")\n",
    "            return float('inf')\n",
    "    \n",
    "    def select_uncertain_points(self):\n",
    "        \"\"\"Step 6: Select new training points based on uncertainty\"\"\"\n",
    "        print(\"\\nStep 6: Calculating uncertainty and selecting new points...\")\n",
    "        print(f\"Calculating uncertainty using rolling prediction (predicting {TRAINING_FRAMES - INPUT_LEN} steps)\")\n",
    "        \n",
    "        # Get points from the pool that have not been selected\n",
    "        available_indices = [\n",
    "            i for i in range(len(self.pool_info))\n",
    "            if i not in self.train_indices and self.pool_info[i]['status'] == 'completed'\n",
    "        ]\n",
    "        \n",
    "        print(f\"Number of available points: {len(available_indices)}\")\n",
    "        \n",
    "        if len(available_indices) == 0:\n",
    "            print(\"Warning: No available points for selection\")\n",
    "            return []\n",
    "        \n",
    "        # Calculate uncertainty for each point\n",
    "        uncertainties = []\n",
    "        for idx in tqdm(available_indices, desc=\"Calculating uncertainty\"):\n",
    "            sim_dir = self.pool_info[idx]['sim_dir']\n",
    "            unc = self.calculate_uncertainty(sim_dir)\n",
    "            if unc != float('inf'):  # Only record valid uncertainties\n",
    "                uncertainties.append((idx, unc))\n",
    "        \n",
    "        if not uncertainties:\n",
    "            print(\"Warning: No valid uncertainties were calculated\")\n",
    "            return []\n",
    "        \n",
    "        # Sort and select TOP_K\n",
    "        uncertainties.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Ensure not to exceed the number of available points\n",
    "        n_select = min(TOP_K, len(uncertainties))\n",
    "        selected_indices = [idx for idx, _ in uncertainties[:n_select]]\n",
    "        \n",
    "        print(f\"Selected new point indices: {selected_indices}\")\n",
    "        print(f\"Corresponding uncertainties: {[unc for _, unc in uncertainties[:n_select]]}\")\n",
    "        \n",
    "        return selected_indices\n",
    "    \n",
    "    def evaluate_ensemble(self):\n",
    "        \"\"\"Evaluate ensemble model performance\"\"\"\n",
    "        print(\"\\nEvaluating ensemble model...\")\n",
    "        \n",
    "        # Load test data\n",
    "        test_mats, _ = self.load_data(self.test_dirs)\n",
    "        X_test, Y_test = self.create_windows(test_mats)\n",
    "        \n",
    "        if len(X_test) == 0:\n",
    "            print(\"Test data is empty, skipping evaluation\")\n",
    "            return None\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            TimeSeriesDataset(X_test, Y_test),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        # Ensemble prediction\n",
    "        criterion = nn.MSELoss()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_loader:\n",
    "                # Get predictions from all models\n",
    "                predictions = []\n",
    "                for model in self.ensemble_models:\n",
    "                    model.eval()\n",
    "                    pred = model(x_batch)\n",
    "                    predictions.append(pred)\n",
    "                \n",
    "                # Average predictions\n",
    "                ensemble_pred = torch.stack(predictions).mean(dim=0)\n",
    "                \n",
    "                # Calculate loss\n",
    "                loss = criterion(ensemble_pred, y_batch)\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        print(f\"Ensemble model test set MSE: {avg_loss:.6f}\")\n",
    "        \n",
    "        # Save evaluation results\n",
    "        eval_result = {\n",
    "            'iteration': self.iteration,\n",
    "            'test_mse': avg_loss,\n",
    "            'train_size': len(self.train_indices),\n",
    "            'ensemble_size': len(self.ensemble_models)\n",
    "        }\n",
    "        \n",
    "        eval_file = os.path.join(self.results_dir, f'evaluation_iter_{self.iteration}.json')\n",
    "        with open(eval_file, 'w') as f:\n",
    "            json.dump(eval_result, f, indent=2)\n",
    "        \n",
    "        return avg_loss\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the main active learning loop\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"Starting Active Learning Process\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Step 3: Select initial training set\n",
    "        self.select_initial_training_set()\n",
    "        \n",
    "        # Active learning iterations\n",
    "        for iter_idx in range(N_ITERATIONS):\n",
    "            self.iteration = iter_idx\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Iteration {iter_idx + 1}/{N_ITERATIONS}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            # Step 4: Run extended simulations (if needed)\n",
    "            self.run_extended_simulation(self.train_indices)\n",
    "            \n",
    "            # Find common species\n",
    "            self.find_common_species()\n",
    "            \n",
    "            # Step 5: Train ensemble models\n",
    "            self.train_ensemble_models()\n",
    "            \n",
    "            # Evaluate models\n",
    "            test_loss = self.evaluate_ensemble()\n",
    "            \n",
    "            # If not the last iteration, select new points\n",
    "            if iter_idx < N_ITERATIONS - 1:\n",
    "                # Step 6: Select new uncertain points\n",
    "                new_indices = self.select_uncertain_points()\n",
    "                \n",
    "                if new_indices:  # Only add if new points were selected\n",
    "                    # Add new points to the training set\n",
    "                    self.train_indices.extend(new_indices)\n",
    "                    print(f\"Training set size: {len(self.train_indices)}\")\n",
    "                else:\n",
    "                    print(\"Warning: No new points were selected, continuing to next iteration\")\n",
    "                \n",
    "                # Save current state\n",
    "                state = {\n",
    "                    'iteration': iter_idx,\n",
    "                    'train_indices': self.train_indices,\n",
    "                    'test_loss': test_loss,\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "                \n",
    "                state_file = os.path.join(self.results_dir, f'state_iter_{iter_idx}.json')\n",
    "                with open(state_file, 'w') as f:\n",
    "                    json.dump(state, f, indent=2)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Active learning complete!\")\n",
    "        print(f\"Final training set size: {len(self.train_indices)}\")\n",
    "        print(f\"Results saved in: {self.results_dir}\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function\"\"\"\n",
    "    try:\n",
    "        # --- NEW ---: Set the global random seed at the start of the program\n",
    "        set_seed(RANDOM_SEED)\n",
    "\n",
    "        # Create an instance of the ActiveLearningFramework\n",
    "        framework = ActiveLearningFramework()\n",
    "        \n",
    "        # Run active learning\n",
    "        framework.run()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15ab44e-147b-4ace-8a34-6ba475ee0c7d",
   "metadata": {},
   "source": [
    "### Continue running AL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452bf428-5d1b-444f-8b06-be5ab640cf0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Resume Function (Jupyter friendly, no main required) =====\n",
    "import os, re, glob, json\n",
    "from datetime import datetime\n",
    "\n",
    "def _safe_int(x, default=-1):\n",
    "    try:\n",
    "        return int(x)\n",
    "    except Exception:\n",
    "        return default\n",
    "\n",
    "def _load_train_indices_from_results(self, prev_results_dir: str):\n",
    "    \"\"\"\n",
    "    Read the selected training set indices and the last iteration number from the previous results directory.\n",
    "    Prioritizes reading state_iter_*.json; falls back to initial_train_set.json if not found.\n",
    "    \"\"\"\n",
    "    if not os.path.isdir(prev_results_dir):\n",
    "        raise FileNotFoundError(f\"Resume directory not found: {prev_results_dir}\")\n",
    "\n",
    "    # 1) Priority: Read the latest state_iter_*.json\n",
    "    state_files = glob.glob(os.path.join(prev_results_dir, 'state_iter_*.json'))\n",
    "    if state_files:\n",
    "        def iternum(p):\n",
    "            m = re.search(r'state_iter_(\\d+)\\.json', os.path.basename(p))\n",
    "            return _safe_int(m.group(1) if m else -1, -1)\n",
    "        latest_state = max(state_files, key=iternum)\n",
    "        with open(latest_state, 'r') as f:\n",
    "            state = json.load(f)\n",
    "        train_indices = state.get('train_indices', [])\n",
    "        last_iter = state.get('iteration', -1)\n",
    "        if not train_indices:\n",
    "            raise RuntimeError(f\"train_indices not found in {latest_state}\")\n",
    "        print(f\"Loaded training set from {os.path.basename(latest_state)}, size {len(train_indices)}, last iteration number {last_iter}\")\n",
    "        return train_indices, last_iter\n",
    "\n",
    "    # 2) Fallback: Only initial set (requires index reverse lookup)\n",
    "    init_file = os.path.join(prev_results_dir, 'initial_train_set.json')\n",
    "    if os.path.exists(init_file):\n",
    "        with open(init_file, 'r') as f:\n",
    "            init_list = json.load(f)\n",
    "        # Extract sim_dir from items\n",
    "        init_sim_dirs = set()\n",
    "        for it in init_list:\n",
    "            if isinstance(it, dict):\n",
    "                # Common field sim_dir\n",
    "                if 'sim_dir' in it:\n",
    "                    init_sim_dirs.add(os.path.abspath(it['sim_dir']))\n",
    "                # If your structure is different, you can add other fields for reverse lookup here\n",
    "        if not init_sim_dirs:\n",
    "            raise RuntimeError(f\"No usable sim_dir field in {init_file}, cannot reverse lookup indices\")\n",
    "\n",
    "        # Reverse lookup indices based on current self.pool_info\n",
    "        idxs = []\n",
    "        for i, info in enumerate(self.pool_info):\n",
    "            sd = os.path.abspath(info.get('sim_dir', ''))\n",
    "            if sd in init_sim_dirs:\n",
    "                idxs.append(i)\n",
    "        if not idxs:\n",
    "            raise RuntimeError(\"Cannot reverse lookup any indices from initial_train_set.json, please check if pool_info and results directory correspond\")\n",
    "        print(f\"state_iter_*.json not found, falling back to initial set, size {len(idxs)}\")\n",
    "        return idxs, -1\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        f\"Neither state_iter_*.json nor initial_train_set.json found in {prev_results_dir}, cannot resume\"\n",
    "    )\n",
    "\n",
    "def resume(self, prev_results_dir: str, extra_iterations: int = 50):\n",
    "    \"\"\"\n",
    "    Read the selected training set from an existing results directory (prev_results_dir)\n",
    "    and continue active learning for extra_iterations rounds.\n",
    "    Resume output is saved in the current instance's self.results_dir (timestamped).\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"Starting resume: Continuing from {prev_results_dir} for {extra_iterations} rounds\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # Read last training set indices\n",
    "    loaded_indices, last_iter_prev = self._load_train_indices_from_results(prev_results_dir)\n",
    "    self.train_indices = list(loaded_indices)  # Copy\n",
    "    print(f\"Training set size at resume start: {len(self.train_indices)}\")\n",
    "\n",
    "    # Record source\n",
    "    resume_meta = {\n",
    "        'resume_from': os.path.abspath(prev_results_dir),\n",
    "        'loaded_last_iter': last_iter_prev,\n",
    "        'start_train_size': len(self.train_indices),\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    with open(os.path.join(self.results_dir, 'resume_from.json'), 'w') as f:\n",
    "        json.dump(resume_meta, f, indent=2)\n",
    "\n",
    "    # Enter resume loop\n",
    "    start_offset = (last_iter_prev + 1) if (last_iter_prev is not None and last_iter_prev >= 0) else 0\n",
    "    for k in range(extra_iterations):\n",
    "        self.iteration = start_offset + k\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Resume iteration {k + 1}/{extra_iterations} (Global number {self.iteration})\")\n",
    "        print(f\"{'='*60}\")\n",
    "\n",
    "        # Follow original flow:\n",
    "        self.run_extended_simulation(self.train_indices)\n",
    "        self.find_common_species()\n",
    "        self.train_ensemble_models()\n",
    "        test_loss = self.evaluate_ensemble()\n",
    "\n",
    "        # Select new points and expand training set\n",
    "        new_indices = self.select_uncertain_points()\n",
    "        if new_indices:\n",
    "            self.train_indices.extend(new_indices)\n",
    "            print(f\"Training set size after resume: {len(self.train_indices)}\")\n",
    "        else:\n",
    "            print(\"Note: No new points selected this round (perhaps no available completed points in the pool)\")\n",
    "\n",
    "        # Save state\n",
    "        state = {\n",
    "            'iteration': self.iteration,\n",
    "            'train_indices': self.train_indices,\n",
    "            'test_loss': test_loss,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "        state_file = os.path.join(self.results_dir, f'state_iter_{self.iteration}.json')\n",
    "        with open(state_file, 'w') as f:\n",
    "            json.dump(state, f, indent=2)\n",
    "\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Resume complete!\")\n",
    "    print(f\"Final training set size: {len(self.train_indices)}\")\n",
    "    print(f\"Resume results saved in: {self.results_dir}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# ---- \"Mount\" the above two methods onto your existing ActiveLearningFramework class ----\n",
    "ActiveLearningFramework._load_train_indices_from_results = _load_train_indices_from_results\n",
    "ActiveLearningFramework.resume = resume\n",
    "\n",
    "# Note: The following lines assume 'ActiveLearningFramework' is defined in a previous cell (e.g., in Jupyter).\n",
    "# framework = ActiveLearningFramework()\n",
    "# framework.resume(prev_results_dir=\"C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/AL_results_20250819_162332\",\n",
    "#                  extra_iterations=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d673dd5c-41bd-4c7d-a61b-11c22a1662d7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 2. Random_search_part "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7ed448-d09f-45d3-8340-76f051814cfb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Main_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44ec28-2441-4f66-ba36-1f60aa0407cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Random Sampling Comparison Test Script\n",
    "Used for performance comparison against the active learning framework\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import subprocess\n",
    "import shutil\n",
    "import random\n",
    "import copy\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "\n",
    "# ============ Configuration Parameters ============\n",
    "BASE_DIR = 'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0'\n",
    "POOL_DATA_DIR = os.path.join(BASE_DIR, 'Pool_data')\n",
    "WORK_PATH = BASE_DIR\n",
    "\n",
    "# Model Parameters (consistent with active learning)\n",
    "TRAINING_FRAMES = 500  # N2 - Number of frames for training\n",
    "INPUT_LEN = 50\n",
    "PRED_LEN = 150\n",
    "STRIDE = 10\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 200\n",
    "LEARNING_RATE = 1e-3\n",
    "ENSEMBLE_SIZE = 10  # Number of ensemble models\n",
    "\n",
    "AL_test_MSE = 36.638477\n",
    "\n",
    "# Test set paths\n",
    "TEST_PATHS = [\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_2atm_1per15',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_3atm_1per17.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_4atm_1per20',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_5atm_1per22.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_6atm_1per25',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_2atm_1per17.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_3atm_1per20',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_4atm_1per22.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_5atm_1per25',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_6atm_1per15',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_2atm_1per20',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_3atm_1per22.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_4atm_1per25',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_5atm_1per15',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_6atm_1per17.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_2atm_1per22.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_3atm_1per25',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_4atm_1per15',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_5atm_1per17.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_6atm_1per20',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_2atm_1per25',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_3atm_1per15',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_4atm_1per17.5',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_5atm_1per20',\n",
    "    'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_6atm_1per22.5',\n",
    "]\n",
    "\n",
    "# ============ TCN Model Definition (consistent with active learning) ============\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = nn.utils.weight_norm(\n",
    "            nn.Conv1d(in_ch, out_ch, kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation)\n",
    "        )\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.conv2 = nn.utils.weight_norm(\n",
    "            nn.Conv1d(out_ch, out_ch, kernel_size, stride=stride,\n",
    "                      padding=padding, dilation=dilation)\n",
    "        )\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        self.net = nn.Sequential(self.conv1, self.relu1, self.dropout1,\n",
    "                                 self.conv2, self.relu2, self.dropout2)\n",
    "        self.downsample = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        if out.size(2) != res.size(2):\n",
    "            out = out[:, :, :res.size(2)]\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=3, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation = 2 ** i\n",
    "            in_ch = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_ch = num_channels[i]\n",
    "            padding = (kernel_size-1) * dilation\n",
    "            layers += [TemporalBlock(in_ch, out_ch, kernel_size, stride=1,\n",
    "                                     dilation=dilation, padding=padding,\n",
    "                                     dropout=dropout)]\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class TCNForecast(nn.Module):\n",
    "    def __init__(self, input_dim, num_channels, kernel_size, pred_len):\n",
    "        super(TCNForecast, self).__init__()\n",
    "        self.tcn = TemporalConvNet(input_dim, num_channels, kernel_size)\n",
    "        self.linear = nn.Linear(num_channels[-1], input_dim * pred_len)\n",
    "        self.input_dim = input_dim\n",
    "        self.pred_len = pred_len\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.transpose(1, 2)\n",
    "        y = self.tcn(x)\n",
    "        out = y[:, :, -1]\n",
    "        pred = self.linear(out)\n",
    "        return pred.view(-1, self.pred_len, self.input_dim)\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.Y = torch.from_numpy(Y).float()\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=1e-4):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "    \n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss - val_loss > self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        return self.early_stop\n",
    "\n",
    "# ============ Random Sampling Test Class ============\n",
    "class RandomSamplingTest:\n",
    "    def __init__(self, train_size=50, num_runs=5, seed=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            train_size: Training set size\n",
    "            num_runs: Number of runs (average over multiple random samples)\n",
    "            seed: Random seed\n",
    "        \"\"\"\n",
    "        self.train_size = train_size\n",
    "        self.num_runs = num_runs\n",
    "        self.seed = seed\n",
    "        self.pool_info = None\n",
    "        self.test_dirs = TEST_PATHS\n",
    "        self.ensemble_models = []\n",
    "        \n",
    "        # Create results directory\n",
    "        self.results_dir = os.path.join(\n",
    "            BASE_DIR, \n",
    "            f'Random_Sampling_Results_{datetime.now().strftime(\"%Y%m%d_%H%M%S\")}'\n",
    "        )\n",
    "        os.makedirs(self.results_dir, exist_ok=True)\n",
    "        \n",
    "        # Load pool information\n",
    "        self.load_pool_info()\n",
    "    \n",
    "    def load_pool_info(self):\n",
    "        \"\"\"Load parameter pool information\"\"\"\n",
    "        pool_info_file = os.path.join(BASE_DIR, 'pool_info.json')\n",
    "        if not os.path.exists(pool_info_file):\n",
    "            raise FileNotFoundError(f\"Pool info file not found: {pool_info_file}\")\n",
    "        \n",
    "        with open(pool_info_file, 'r') as f:\n",
    "            self.pool_info = json.load(f)\n",
    "        \n",
    "        print(f\"Loaded {len(self.pool_info)} parameter point infos\")\n",
    "    \n",
    "    def check_simulation_frames(self, sim_dir):\n",
    "        \"\"\"Check if simulation has reached TRAINING_FRAMES\"\"\"\n",
    "        # Check the frame count of species_time_matrix_initial.npy\n",
    "        mat_path = os.path.join(sim_dir, 'species_time_matrix_initial.npy')\n",
    "        if os.path.exists(mat_path):\n",
    "            mat = np.load(mat_path)\n",
    "            return mat.shape[1] >= TRAINING_FRAMES\n",
    "        return False\n",
    "    \n",
    "    def run_extended_simulation(self, sim_dir, info):\n",
    "        \"\"\"Run extended simulation to reach TRAINING_FRAMES\"\"\"\n",
    "        print(f\"  Extending simulation: {os.path.basename(sim_dir)}\")\n",
    "        \n",
    "        # Modify in.MoO3S file\n",
    "        in_path = os.path.join(sim_dir, \"in.MoO3S\")\n",
    "        \n",
    "        # Calculate new total steps\n",
    "        steps_per_frame = 5000\n",
    "        new_run_steps = steps_per_frame * TRAINING_FRAMES\n",
    "        \n",
    "        # Read and modify file\n",
    "        new_lines = []\n",
    "        with open(in_path, 'r') as f:\n",
    "            for line in f:\n",
    "                if line.strip().startswith(\"run\"):\n",
    "                    line = f\"run {new_run_steps}\\n\"\n",
    "                new_lines.append(line)\n",
    "        \n",
    "        # Write back to file\n",
    "        with open(in_path, 'w') as f:\n",
    "            f.writelines(new_lines)\n",
    "        \n",
    "        # Backup old log file\n",
    "        log_file = os.path.join(sim_dir, \"log.lammps\")\n",
    "        if os.path.exists(log_file):\n",
    "            backup_log = os.path.join(sim_dir, f\"log.lammps.backup_random\")\n",
    "            shutil.move(log_file, backup_log)\n",
    "        \n",
    "        # Run LAMMPS simulation\n",
    "        cmd = (\n",
    "            \"module load lammps/20230328-intel-2021.4.0-omp && \"\n",
    "            f\"mpirun -np 48 lmp -in {in_path}\"\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            result = subprocess.run(\n",
    "                cmd,\n",
    "                cwd=sim_dir,\n",
    "                shell=True,\n",
    "                capture_output=True,\n",
    "                text=True,\n",
    "                timeout=7200\n",
    "            )\n",
    "            \n",
    "            # Run post-processing\n",
    "            proc_script = os.path.join(WORK_PATH, \"lammps_output_process.py\")\n",
    "            if os.path.exists(proc_script):\n",
    "                subprocess.run([\"python\", proc_script, sim_dir], capture_output=True, text=True)\n",
    "            \n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"  Extended simulation failed: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def random_sample_train_set(self, run_idx):\n",
    "        \"\"\"Randomly select training set\"\"\"\n",
    "        print(f\"\\nRun {run_idx+1}/{self.num_runs}: Randomly selecting training set...\")\n",
    "        \n",
    "        # Select only completed points\n",
    "        completed_indices = [\n",
    "            i for i, info in enumerate(self.pool_info)\n",
    "            if info['status'] == 'completed'\n",
    "        ]\n",
    "        \n",
    "        if len(completed_indices) < self.train_size:\n",
    "            raise ValueError(f\"Number of completed simulation points ({len(completed_indices)}) is less than training set size ({self.train_size})\")\n",
    "        \n",
    "        # Set random seed\n",
    "        if self.seed is not None:\n",
    "            random.seed(self.seed + run_idx)\n",
    "        \n",
    "        # Randomly select\n",
    "        train_indices = random.sample(completed_indices, self.train_size)\n",
    "        print(f\"Selected training point indices: {train_indices}\")\n",
    "        \n",
    "        return train_indices\n",
    "    \n",
    "    def ensure_training_frames(self, train_indices):\n",
    "        \"\"\"Ensure all training points reach TRAINING_FRAMES\"\"\"\n",
    "        print(\"\\nChecking and extending simulations to required frame count...\")\n",
    "        \n",
    "        for idx in train_indices:\n",
    "            info = self.pool_info[idx]\n",
    "            sim_dir = info['sim_dir']\n",
    "            \n",
    "            if not self.check_simulation_frames(sim_dir):\n",
    "                print(f\"Extension needed: {os.path.basename(sim_dir)}\")\n",
    "                success = self.run_extended_simulation(sim_dir, info)\n",
    "                if not success:\n",
    "                    print(f\"Warning: Extension failed - {sim_dir}\")\n",
    "    \n",
    "    def find_common_species(self, train_indices):\n",
    "        \"\"\"Find common species\"\"\"\n",
    "        print(\"\\nFinding common species...\")\n",
    "        \n",
    "        # Prepare all paths\n",
    "        all_paths = []\n",
    "        \n",
    "        # Training set paths\n",
    "        for idx in train_indices:\n",
    "            sim_dir = self.pool_info[idx]['sim_dir']\n",
    "            list_file = os.path.join(sim_dir, 'species_list_initial.txt')\n",
    "            all_paths.append(list_file)\n",
    "        \n",
    "        # Test set paths\n",
    "        for test_dir in self.test_dirs:\n",
    "            list_file = os.path.join(test_dir, 'species_list_initial.txt')\n",
    "            all_paths.append(list_file)\n",
    "        \n",
    "        # Call found_species_subsets.py script\n",
    "        subset_script = os.path.join(WORK_PATH, 'found_species_subsets.py')\n",
    "        if os.path.exists(subset_script):\n",
    "            cmd = ['python', subset_script] + all_paths\n",
    "            subprocess.run(cmd, capture_output=True, text=True)\n",
    "            print(\"True subset finding complete\")\n",
    "    \n",
    "    def load_data(self, dir_list, max_frames=TRAINING_FRAMES):\n",
    "        \"\"\"Load data\"\"\"\n",
    "        mats = []\n",
    "        species_lists = []\n",
    "        \n",
    "        for d in dir_list:\n",
    "            mat_path = os.path.join(d, 'species_time_matrix.npy')\n",
    "            list_path = os.path.join(d, 'species_list.txt')\n",
    "            \n",
    "            if not os.path.exists(mat_path):\n",
    "                mat_path = os.path.join(d, 'species_time_matrix_initial.npy')\n",
    "                list_path = os.path.join(d, 'species_list_initial.txt')\n",
    "            \n",
    "            if os.path.exists(mat_path) and os.path.exists(list_path):\n",
    "                mat = np.load(mat_path)\n",
    "                mats.append(mat[:, :max_frames])\n",
    "                with open(list_path, 'r') as f:\n",
    "                    species_lists.append([line.strip() for line in f if line.strip()])\n",
    "        \n",
    "        return mats, species_lists\n",
    "    \n",
    "    def create_windows(self, matrices):\n",
    "        \"\"\"Create sliding window data\"\"\"\n",
    "        X_list, Y_list = [], []\n",
    "        \n",
    "        for mat in matrices:\n",
    "            num_species, T = mat.shape\n",
    "            for start in range(0, T - INPUT_LEN - PRED_LEN + 1, STRIDE):\n",
    "                x = mat[:, start: start + INPUT_LEN]\n",
    "                y = mat[:, start + INPUT_LEN: start + INPUT_LEN + PRED_LEN]\n",
    "                X_list.append(x.T)\n",
    "                Y_list.append(y.T)\n",
    "        \n",
    "        if X_list:\n",
    "            return np.stack(X_list), np.stack(Y_list)\n",
    "        else:\n",
    "            return np.array([]), np.array([])\n",
    "    \n",
    "    def train_ensemble(self, X_train, Y_train, X_test, Y_test, run_idx):\n",
    "        \"\"\"Train ensemble models\"\"\"\n",
    "        print(\"\\nTraining ensemble TCN models...\")\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            TimeSeriesDataset(X_train, Y_train),\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=True\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            TimeSeriesDataset(X_test, Y_test),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        self.ensemble_models = []\n",
    "        input_dim = X_train.shape[2]\n",
    "        hidden_channels = [64, 64, 64]\n",
    "        kernel_size = 3\n",
    "        \n",
    "        for model_idx in range(ENSEMBLE_SIZE):\n",
    "            print(f\"Training model {model_idx + 1}/{ENSEMBLE_SIZE}\")\n",
    "            \n",
    "            model = TCNForecast(input_dim, hidden_channels, kernel_size, PRED_LEN)\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "            criterion = nn.MSELoss()\n",
    "            early_stopping = EarlyStopping(patience=10)\n",
    "            \n",
    "            best_val_loss = float('inf')\n",
    "            best_state_dict = None\n",
    "            \n",
    "            for epoch in range(1, EPOCHS + 1):\n",
    "                # Train\n",
    "                model.train()\n",
    "                total_loss = 0.0\n",
    "                for x_batch, y_batch in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = criterion(model(x_batch), y_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_loss += loss.item()\n",
    "                \n",
    "                avg_train = total_loss / len(train_loader)\n",
    "                \n",
    "                # Validate\n",
    "                model.eval()\n",
    "                val_loss = 0.0\n",
    "                with torch.no_grad():\n",
    "                    for x_batch, y_batch in test_loader:\n",
    "                        val_loss += criterion(model(x_batch), y_batch).item()\n",
    "                \n",
    "                avg_val = val_loss / len(test_loader)\n",
    "                \n",
    "                if epoch % 20 == 0:\n",
    "                    print(f\"  Epoch {epoch}: Train Loss={avg_train:.4f}, Val Loss={avg_val:.4f}\")\n",
    "                \n",
    "                if avg_val < best_val_loss:\n",
    "                    best_val_loss = avg_val\n",
    "                    best_state_dict = copy.deepcopy(model.state_dict())\n",
    "                \n",
    "                if early_stopping(avg_val):\n",
    "                    print(f\"  Early stopping at epoch {epoch}\")\n",
    "                    break\n",
    "            \n",
    "            if best_state_dict is not None:\n",
    "                model.load_state_dict(best_state_dict)\n",
    "            \n",
    "            # Save model\n",
    "            model_path = os.path.join(\n",
    "                self.results_dir,\n",
    "                f'model_run{run_idx}_model{model_idx}.pth'\n",
    "            )\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            \n",
    "            self.ensemble_models.append(model)\n",
    "    \n",
    "    def evaluate_ensemble(self, X_test, Y_test):\n",
    "        \"\"\"Evaluate ensemble model\"\"\"\n",
    "        print(\"\\nEvaluating ensemble model...\")\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            TimeSeriesDataset(X_test, Y_test),\n",
    "            batch_size=BATCH_SIZE\n",
    "        )\n",
    "        \n",
    "        criterion = nn.MSELoss()\n",
    "        total_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in test_loader:\n",
    "                predictions = []\n",
    "                for model in self.ensemble_models:\n",
    "                    model.eval()\n",
    "                    pred = model(x_batch)\n",
    "                    predictions.append(pred)\n",
    "                \n",
    "                ensemble_pred = torch.stack(predictions).mean(dim=0)\n",
    "                loss = criterion(ensemble_pred, y_batch)\n",
    "                total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(test_loader)\n",
    "        print(f\"Ensemble model test set MSE: {avg_loss:.6f}\")\n",
    "        \n",
    "        return avg_loss\n",
    "    \n",
    "    def run_single_test(self, run_idx):\n",
    "        \"\"\"Run a single test\"\"\"\n",
    "        # Randomly select training set\n",
    "        train_indices = self.random_sample_train_set(run_idx)\n",
    "        \n",
    "        # Ensure all points reach TRAINING_FRAMES\n",
    "        self.ensure_training_frames(train_indices)\n",
    "        \n",
    "        # Find common species\n",
    "        self.find_common_species(train_indices)\n",
    "        \n",
    "        # Load data\n",
    "        train_dirs = [self.pool_info[i]['sim_dir'] for i in train_indices]\n",
    "        train_mats, _ = self.load_data(train_dirs)\n",
    "        test_mats, _ = self.load_data(self.test_dirs)\n",
    "        \n",
    "        # Create window data\n",
    "        X_train, Y_train = self.create_windows(train_mats)\n",
    "        X_test, Y_test = self.create_windows(test_mats)\n",
    "        \n",
    "        if len(X_train) == 0 or len(X_test) == 0:\n",
    "            print(\"Warning: Data is empty\")\n",
    "            return None, train_indices\n",
    "        \n",
    "        print(f\"Training data shape: X={X_train.shape}, Y={Y_train.shape}\")\n",
    "        print(f\"Test data shape: X={X_test.shape}, Y={Y_test.shape}\")\n",
    "        \n",
    "        # Train ensemble models\n",
    "        self.train_ensemble(X_train, Y_train, X_test, Y_test, run_idx)\n",
    "        \n",
    "        # Evaluate model\n",
    "        test_mse = self.evaluate_ensemble(X_test, Y_test)\n",
    "        \n",
    "        return test_mse, train_indices\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Run the full test\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(f\"Random Sampling Comparison Test\")\n",
    "        print(f\"Training set size: {self.train_size}\")\n",
    "        print(f\"Number of runs: {self.num_runs}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        all_results = []\n",
    "        \n",
    "        for run_idx in range(self.num_runs):\n",
    "            print(f\"\\n{'='*40}\")\n",
    "            print(f\"Run {run_idx+1}/{self.num_runs}\")\n",
    "            print(f\"{'='*40}\")\n",
    "            \n",
    "            test_mse, train_indices = self.run_single_test(run_idx)\n",
    "            \n",
    "            if test_mse is not None:\n",
    "                result = {\n",
    "                    'run': run_idx + 1,\n",
    "                    'test_mse': test_mse,\n",
    "                    'train_indices': train_indices,\n",
    "                    'train_size': self.train_size\n",
    "                }\n",
    "                all_results.append(result)\n",
    "                \n",
    "                # Save single run result\n",
    "                result_file = os.path.join(\n",
    "                    self.results_dir,\n",
    "                    f'result_run_{run_idx+1}.json'\n",
    "                )\n",
    "                with open(result_file, 'w') as f:\n",
    "                    json.dump(result, f, indent=2)\n",
    "        \n",
    "        # Statistical analysis\n",
    "        if all_results:\n",
    "            mse_values = [r['test_mse'] for r in all_results]\n",
    "            mean_mse = np.mean(mse_values)\n",
    "            std_mse = np.std(mse_values)\n",
    "            min_mse = np.min(mse_values)\n",
    "            max_mse = np.max(mse_values)\n",
    "            \n",
    "            summary = {\n",
    "                'train_size': self.train_size,\n",
    "                'num_runs': self.num_runs,\n",
    "                'mean_mse': mean_mse,\n",
    "                'std_mse': std_mse,\n",
    "                'min_mse': min_mse,\n",
    "                'max_mse': max_mse,\n",
    "                'all_mse_values': mse_values,\n",
    "                'all_results': all_results\n",
    "            }\n",
    "            \n",
    "            # Save summary\n",
    "            summary_file = os.path.join(self.results_dir, 'summary.json')\n",
    "            with open(summary_file, 'w') as f:\n",
    "                json.dump(summary, f, indent=2)\n",
    "            \n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"Test Complete - Statistical Results\")\n",
    "            print(\"=\"*60)\n",
    "            print(f\"Mean MSE: {mean_mse:.6f}  {std_mse:.6f}\")\n",
    "            print(f\"Min MSE: {min_mse:.6f}\")\n",
    "            print(f\"Max MSE: {max_mse:.6f}\")\n",
    "            print(f\"Results saved in: {self.results_dir}\")\n",
    "            \n",
    "            # Comparison with Active Learning result\n",
    "            print(\"\\n\" + \"=\"*60)\n",
    "            print(\"Comparison with Active Learning:\")\n",
    "            print(f\"Random Sampling MSE: {mean_mse:.6f}  {std_mse:.6f} ({self.train_size} points)\")\n",
    "            \n",
    "            if mean_mse < AL_test_MSE:\n",
    "                improvement = (AL_test_MSE - mean_mse) / AL_test_MSE * 100\n",
    "                print(f\"Random Sampling is better than Active Learning: {improvement:.2f}% improvement\")\n",
    "            else:\n",
    "                degradation = (mean_mse - AL_test_MSE) / AL_test_MSE * 100\n",
    "                print(f\"Active Learning is better than Random Sampling: {degradation:.2f}% improvement\")\n",
    "\n",
    "train_size = 100\n",
    "num_runs = 1\n",
    "seed = 42\n",
    "# Create test instance\n",
    "tester = RandomSamplingTest(\n",
    "    train_size=train_size,\n",
    "    num_runs=num_runs,\n",
    "    seed=seed\n",
    ")\n",
    "\n",
    "# Run test\n",
    "tester.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5db00fb-ce56-40a3-8514-ee9134a37892",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Visualization_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f770a705-6b74-4b2d-96c6-14dd21befba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "Visualization script for parameter space distribution\n",
    "Visualizes pool points distribution and active learning/random sampling selection\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import argparse\n",
    "from datetime import datetime\n",
    "\n",
    "# Base path\n",
    "BASE_DIR = 'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0'\n",
    "\n",
    "# Set matplotlib to use a better font for scientific publications\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans']\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "def load_pool_info():\n",
    "    \"\"\"Load pool information\"\"\"\n",
    "    pool_info_file = os.path.join(BASE_DIR, 'pool_info.json')\n",
    "    with open(pool_info_file, 'r') as f:\n",
    "        pool_info = json.load(f)\n",
    "    return pool_info\n",
    "\n",
    "def load_active_learning_results(results_dir):\n",
    "    \"\"\"Load active learning results including MSE history\"\"\"\n",
    "    train_indices_history = []\n",
    "    mse_history = []\n",
    "    \n",
    "    # Load iteration states\n",
    "    for i in range(100):  # Assume max 100 iterations\n",
    "        state_file = os.path.join(results_dir, f'state_iter_{i}.json')\n",
    "        eval_file = os.path.join(results_dir, f'evaluation_iter_{i}.json')\n",
    "        \n",
    "        if os.path.exists(state_file):\n",
    "            with open(state_file, 'r') as f:\n",
    "                state = json.load(f)\n",
    "                train_indices_history.append(state['train_indices'])\n",
    "        \n",
    "        if os.path.exists(eval_file):\n",
    "            with open(eval_file, 'r') as f:\n",
    "                eval_data = json.load(f)\n",
    "                mse_history.append({\n",
    "                    'iteration': eval_data['iteration'],\n",
    "                    'test_mse': eval_data['test_mse'],\n",
    "                    'train_size': eval_data['train_size']\n",
    "                })\n",
    "    \n",
    "    # Get final training indices\n",
    "    final_indices = train_indices_history[-1] if train_indices_history else []\n",
    "    \n",
    "    # If no iteration states, try loading initial training set\n",
    "    if not train_indices_history:\n",
    "        initial_file = os.path.join(results_dir, 'initial_train_set.json')\n",
    "        if os.path.exists(initial_file):\n",
    "            with open(initial_file, 'r') as f:\n",
    "                initial_set = json.load(f)\n",
    "                final_indices = [info['index'] for info in initial_set]\n",
    "    \n",
    "    return final_indices, train_indices_history, mse_history\n",
    "\n",
    "def load_random_sampling_results(results_dir):\n",
    "    \"\"\"Load random sampling results including MSE statistics\"\"\"\n",
    "    summary_file = os.path.join(results_dir, 'summary.json')\n",
    "    \n",
    "    if not os.path.exists(summary_file):\n",
    "        return [], None\n",
    "    \n",
    "    with open(summary_file, 'r') as f:\n",
    "        summary = json.load(f)\n",
    "    \n",
    "    # Get indices from first run\n",
    "    random_indices = []\n",
    "    if summary.get('all_results'):\n",
    "        random_indices = summary['all_results'][0]['train_indices']\n",
    "    \n",
    "    return random_indices, summary\n",
    "\n",
    "def visualize_3d_distribution(pool_info, selected_indices=None, title=\"Parameter Space Distribution\"):\n",
    "    \"\"\"3D visualization of parameter space distribution\"\"\"\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Extract all points parameters\n",
    "    all_T = [info['T'] for info in pool_info]\n",
    "    all_P = [info['P'] for info in pool_info]\n",
    "    all_ratio = [info['ratio'] for info in pool_info]\n",
    "    \n",
    "    # Plot all pool points (gray)\n",
    "    ax.scatter(all_T, all_P, all_ratio, c='gray', alpha=0.3, s=20, label='Pool points')\n",
    "    \n",
    "    # If there are selected points, highlight in red\n",
    "    if selected_indices:\n",
    "        selected_T = [pool_info[i]['T'] for i in selected_indices if i < len(pool_info)]\n",
    "        selected_P = [pool_info[i]['P'] for i in selected_indices if i < len(pool_info)]\n",
    "        selected_ratio = [pool_info[i]['ratio'] for i in selected_indices if i < len(pool_info)]\n",
    "        \n",
    "        ax.scatter(selected_T, selected_P, selected_ratio, \n",
    "                  c='red', alpha=1.0, s=100, marker='*', label=f'Selected points (n={len(selected_indices)})')\n",
    "    \n",
    "    ax.set_xlabel('Temperature (K)', fontsize=12)\n",
    "    ax.set_ylabel('Pressure (atm)', fontsize=12)\n",
    "    ax.set_zlabel('Ratio (Mo3O9/S2)', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    \n",
    "    # Set viewing angle\n",
    "    ax.view_init(elev=20, azim=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def visualize_2d_projections(pool_info, selected_indices=None, title_prefix=\"\"):\n",
    "    \"\"\"2D projection visualization\"\"\"\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    # Extract all points parameters\n",
    "    all_T = np.array([info['T'] for info in pool_info])\n",
    "    all_P = np.array([info['P'] for info in pool_info])\n",
    "    all_ratio = np.array([info['ratio'] for info in pool_info])\n",
    "    \n",
    "    # T-P projection\n",
    "    axes[0].scatter(all_T, all_P, c='gray', alpha=0.3, s=20, label='Pool points')\n",
    "    if selected_indices:\n",
    "        selected_T = [pool_info[i]['T'] for i in selected_indices if i < len(pool_info)]\n",
    "        selected_P = [pool_info[i]['P'] for i in selected_indices if i < len(pool_info)]\n",
    "        axes[0].scatter(selected_T, selected_P, c='red', alpha=1.0, s=100, \n",
    "                       marker='*', label=f'Selected (n={len(selected_indices)})')\n",
    "    axes[0].set_xlabel('Temperature (K)')\n",
    "    axes[0].set_ylabel('Pressure (atm)')\n",
    "    axes[0].set_title(f'{title_prefix} T-P Projection')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # T-Ratio projection\n",
    "    axes[1].scatter(all_T, all_ratio, c='gray', alpha=0.3, s=20, label='Pool points')\n",
    "    if selected_indices:\n",
    "        selected_ratio = [pool_info[i]['ratio'] for i in selected_indices if i < len(pool_info)]\n",
    "        axes[1].scatter(selected_T, selected_ratio, c='red', alpha=1.0, s=100,\n",
    "                       marker='*', label=f'Selected (n={len(selected_indices)})')\n",
    "    axes[1].set_xlabel('Temperature (K)')\n",
    "    axes[1].set_ylabel('Ratio (Mo3O9/S2)')\n",
    "    axes[1].set_title(f'{title_prefix} T-Ratio Projection')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # P-Ratio projection\n",
    "    axes[2].scatter(all_P, all_ratio, c='gray', alpha=0.3, s=20, label='Pool points')\n",
    "    if selected_indices:\n",
    "        selected_P = [pool_info[i]['P'] for i in selected_indices if i < len(pool_info)]\n",
    "        axes[2].scatter(selected_P, selected_ratio, c='red', alpha=1.0, s=100,\n",
    "                       marker='*', label=f'Selected (n={len(selected_indices)})')\n",
    "    axes[2].set_xlabel('Pressure (atm)')\n",
    "    axes[2].set_ylabel('Ratio (Mo3O9/S2)')\n",
    "    axes[2].set_title(f'{title_prefix} P-Ratio Projection')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def visualize_distribution_statistics(pool_info, selected_indices=None):\n",
    "    \"\"\"Statistical distribution visualization\"\"\"\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    \n",
    "    # Extract parameters\n",
    "    all_T = np.array([info['T'] for info in pool_info])\n",
    "    all_P = np.array([info['P'] for info in pool_info])\n",
    "    all_ratio = np.array([info['ratio'] for info in pool_info])\n",
    "    \n",
    "    # Temperature distribution histogram\n",
    "    axes[0, 0].hist(all_T, bins=20, alpha=0.5, color='gray', label='Pool')\n",
    "    if selected_indices:\n",
    "        selected_T = [pool_info[i]['T'] for i in selected_indices if i < len(pool_info)]\n",
    "        axes[0, 0].hist(selected_T, bins=20, alpha=0.7, color='red', label='Selected')\n",
    "    axes[0, 0].set_xlabel('Temperature (K)')\n",
    "    axes[0, 0].set_ylabel('Count')\n",
    "    axes[0, 0].set_title('Temperature Distribution')\n",
    "    axes[0, 0].legend()\n",
    "    \n",
    "    # Pressure distribution histogram\n",
    "    axes[0, 1].hist(all_P, bins=20, alpha=0.5, color='gray', label='Pool')\n",
    "    if selected_indices:\n",
    "        selected_P = [pool_info[i]['P'] for i in selected_indices if i < len(pool_info)]\n",
    "        axes[0, 1].hist(selected_P, bins=20, alpha=0.7, color='red', label='Selected')\n",
    "    axes[0, 1].set_xlabel('Pressure (atm)')\n",
    "    axes[0, 1].set_ylabel('Count')\n",
    "    axes[0, 1].set_title('Pressure Distribution')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # Ratio distribution histogram\n",
    "    axes[0, 2].hist(all_ratio, bins=20, alpha=0.5, color='gray', label='Pool')\n",
    "    if selected_indices:\n",
    "        selected_ratio = [pool_info[i]['ratio'] for i in selected_indices if i < len(pool_info)]\n",
    "        axes[0, 2].hist(selected_ratio, bins=20, alpha=0.7, color='red', label='Selected')\n",
    "    axes[0, 2].set_xlabel('Ratio (Mo3O9/S2)')\n",
    "    axes[0, 2].set_ylabel('Count')\n",
    "    axes[0, 2].set_title('Ratio Distribution')\n",
    "    axes[0, 2].legend()\n",
    "    \n",
    "    # Cumulative distribution functions\n",
    "    params = [(all_T, 'Temperature'), (all_P, 'Pressure'), (all_ratio, 'Ratio')]\n",
    "    for idx, (data, name) in enumerate(params):\n",
    "        sorted_data = np.sort(data)\n",
    "        cumulative = np.arange(1, len(sorted_data) + 1) / len(sorted_data)\n",
    "        axes[1, idx].plot(sorted_data, cumulative, 'gray', alpha=0.7, linewidth=2, label='Pool')\n",
    "        \n",
    "        if selected_indices:\n",
    "            if idx == 0:\n",
    "                selected_data = [pool_info[i]['T'] for i in selected_indices if i < len(pool_info)]\n",
    "            elif idx == 1:\n",
    "                selected_data = [pool_info[i]['P'] for i in selected_indices if i < len(pool_info)]\n",
    "            else:\n",
    "                selected_data = [pool_info[i]['ratio'] for i in selected_indices if i < len(pool_info)]\n",
    "            \n",
    "            if selected_data:\n",
    "                sorted_selected = np.sort(selected_data)\n",
    "                cumulative_selected = np.arange(1, len(sorted_selected) + 1) / len(sorted_selected)\n",
    "                axes[1, idx].plot(sorted_selected, cumulative_selected, 'r-', linewidth=2, label='Selected')\n",
    "        \n",
    "        axes[1, idx].set_xlabel(f'{name} {\"(K)\" if name == \"Temperature\" else \"(atm)\" if name == \"Pressure\" else \"\"}')\n",
    "        axes[1, idx].set_ylabel('Cumulative Probability')\n",
    "        axes[1, idx].set_title(f'{name} Cumulative Distribution')\n",
    "        axes[1, idx].legend()\n",
    "        axes[1, idx].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def visualize_mse_evolution(al_mse_history=None, random_summary=None):\n",
    "    \"\"\"Visualize MSE evolution during training\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Active Learning MSE evolution\n",
    "    if al_mse_history:\n",
    "        iterations = [d['iteration'] for d in al_mse_history]\n",
    "        mse_values = [d['test_mse'] for d in al_mse_history]\n",
    "        train_sizes = [d['train_size'] for d in al_mse_history]\n",
    "        \n",
    "        # MSE vs Iteration\n",
    "        ax1 = axes[0]\n",
    "        color = 'tab:blue'\n",
    "        ax1.set_xlabel('Iteration')\n",
    "        ax1.set_ylabel('Test MSE', color=color)\n",
    "        ax1.plot(iterations, mse_values, 'o-', color=color, linewidth=2, markersize=8)\n",
    "        ax1.tick_params(axis='y', labelcolor=color)\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add training size on secondary axis\n",
    "        ax2 = ax1.twinx()\n",
    "        color = 'tab:orange'\n",
    "        ax2.set_ylabel('Training Set Size', color=color)\n",
    "        ax2.plot(iterations, train_sizes, 's--', color=color, linewidth=1.5, markersize=6)\n",
    "        ax2.tick_params(axis='y', labelcolor=color)\n",
    "        \n",
    "        ax1.set_title('Active Learning: MSE Evolution')\n",
    "        \n",
    "        # MSE vs Training Size\n",
    "        axes[1].plot(train_sizes, mse_values, 'o-', color='darkblue', linewidth=2, markersize=8)\n",
    "        axes[1].set_xlabel('Training Set Size')\n",
    "        axes[1].set_ylabel('Test MSE')\n",
    "        axes[1].set_title('Active Learning: MSE vs Training Size')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add final MSE annotation\n",
    "        final_mse = mse_values[-1] if mse_values else 0\n",
    "        final_size = train_sizes[-1] if train_sizes else 0\n",
    "        axes[1].annotate(f'Final: MSE={final_mse:.4f}\\nn={final_size}',\n",
    "                        xy=(final_size, final_mse),\n",
    "                        xytext=(10, 10), textcoords='offset points',\n",
    "                        bbox=dict(boxstyle='round,pad=0.5', fc='yellow', alpha=0.7),\n",
    "                        arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    \n",
    "    # Random Sampling MSE statistics\n",
    "    if random_summary:\n",
    "        ax = axes[1]\n",
    "        train_size = random_summary['train_size']\n",
    "        mean_mse = random_summary['mean_mse']\n",
    "        std_mse = random_summary['std_mse']\n",
    "        \n",
    "        # Add horizontal line for random sampling\n",
    "        ax.axhline(y=mean_mse, color='green', linestyle='--', linewidth=2, label=f'Random Sampling (n={train_size})')\n",
    "        ax.fill_between([0, train_size], \n",
    "                        [mean_mse - std_mse] * 2, \n",
    "                        [mean_mse + std_mse] * 2,\n",
    "                        color='green', alpha=0.2)\n",
    "        ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def visualize_selection_progression(pool_info, train_indices_history):\n",
    "    \"\"\"Visualize how points are selected over iterations\"\"\"\n",
    "    if not train_indices_history:\n",
    "        return None\n",
    "    \n",
    "    n_iterations = len(train_indices_history)\n",
    "    fig, axes = plt.subplots(2, min(3, n_iterations), figsize=(15, 10))\n",
    "    \n",
    "    if n_iterations == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    elif n_iterations == 2:\n",
    "        axes = np.column_stack([axes[:, 0], axes[:, 1], [None, None]])\n",
    "    \n",
    "    # Show first, middle, and last iterations\n",
    "    iterations_to_show = []\n",
    "    if n_iterations <= 3:\n",
    "        iterations_to_show = list(range(n_iterations))\n",
    "    else:\n",
    "        iterations_to_show = [0, n_iterations // 2, n_iterations - 1]\n",
    "    \n",
    "    for plot_idx, iter_idx in enumerate(iterations_to_show):\n",
    "        if iter_idx >= len(train_indices_history):\n",
    "            continue\n",
    "            \n",
    "        selected_indices = train_indices_history[iter_idx]\n",
    "        \n",
    "        # Extract parameters\n",
    "        all_T = [info['T'] for info in pool_info]\n",
    "        all_P = [info['P'] for info in pool_info]\n",
    "        all_ratio = [info['ratio'] for info in pool_info]\n",
    "        \n",
    "        # 3D subplot\n",
    "        ax = axes[0, plot_idx]\n",
    "        ax = fig.add_subplot(2, 3, plot_idx + 1, projection='3d')\n",
    "        ax.scatter(all_T, all_P, all_ratio, c='gray', alpha=0.2, s=10)\n",
    "        \n",
    "        if selected_indices:\n",
    "            sel_T = [pool_info[i]['T'] for i in selected_indices if i < len(pool_info)]\n",
    "            sel_P = [pool_info[i]['P'] for i in selected_indices if i < len(pool_info)]\n",
    "            sel_ratio = [pool_info[i]['ratio'] for i in selected_indices if i < len(pool_info)]\n",
    "            ax.scatter(sel_T, sel_P, sel_ratio, c='red', alpha=1.0, s=50, marker='*')\n",
    "        \n",
    "        ax.set_title(f'Iteration {iter_idx + 1} (n={len(selected_indices)})')\n",
    "        ax.set_xlabel('T (K)', fontsize=9)\n",
    "        ax.set_ylabel('P (atm)', fontsize=9)\n",
    "        ax.set_zlabel('Ratio', fontsize=9)\n",
    "        ax.view_init(elev=20, azim=45)\n",
    "        \n",
    "        # 2D T-P projection\n",
    "        ax2 = axes[1, plot_idx]\n",
    "        if ax2 is not None:\n",
    "            ax2.scatter(all_T, all_P, c='gray', alpha=0.2, s=10)\n",
    "            if selected_indices:\n",
    "                ax2.scatter(sel_T, sel_P, c='red', alpha=1.0, s=50, marker='*')\n",
    "            ax2.set_xlabel('Temperature (K)')\n",
    "            ax2.set_ylabel('Pressure (atm)')\n",
    "            ax2.set_title(f'T-P Projection (Iter {iter_idx + 1})')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.suptitle('Active Learning Selection Progression', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def compare_sampling_methods(pool_info, al_indices, random_indices):\n",
    "    \"\"\"Compare active learning and random sampling distributions\"\"\"\n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # 3D subplot - Active Learning\n",
    "    ax1 = fig.add_subplot(131, projection='3d')\n",
    "    all_T = [info['T'] for info in pool_info]\n",
    "    all_P = [info['P'] for info in pool_info]\n",
    "    all_ratio = [info['ratio'] for info in pool_info]\n",
    "    \n",
    "    ax1.scatter(all_T, all_P, all_ratio, c='gray', alpha=0.2, s=10)\n",
    "    if al_indices:\n",
    "        al_T = [pool_info[i]['T'] for i in al_indices if i < len(pool_info)]\n",
    "        al_P = [pool_info[i]['P'] for i in al_indices if i < len(pool_info)]\n",
    "        al_ratio = [pool_info[i]['ratio'] for i in al_indices if i < len(pool_info)]\n",
    "        ax1.scatter(al_T, al_P, al_ratio, c='blue', alpha=1.0, s=100, marker='^')\n",
    "    ax1.set_title(f'Active Learning (n={len(al_indices)})')\n",
    "    ax1.set_xlabel('T (K)')\n",
    "    ax1.set_ylabel('P (atm)')\n",
    "    ax1.set_zlabel('Ratio')\n",
    "    \n",
    "    # 3D subplot - Random Sampling\n",
    "    ax2 = fig.add_subplot(132, projection='3d')\n",
    "    ax2.scatter(all_T, all_P, all_ratio, c='gray', alpha=0.2, s=10)\n",
    "    if random_indices:\n",
    "        rand_T = [pool_info[i]['T'] for i in random_indices if i < len(pool_info)]\n",
    "        rand_P = [pool_info[i]['P'] for i in random_indices if i < len(pool_info)]\n",
    "        rand_ratio = [pool_info[i]['ratio'] for i in random_indices if i < len(pool_info)]\n",
    "        ax2.scatter(rand_T, rand_P, rand_ratio, c='green', alpha=1.0, s=100, marker='o')\n",
    "    ax2.set_title(f'Random Sampling (n={len(random_indices)})')\n",
    "    ax2.set_xlabel('T (K)')\n",
    "    ax2.set_ylabel('P (atm)')\n",
    "    ax2.set_zlabel('Ratio')\n",
    "    \n",
    "    # Coverage comparison\n",
    "    ax3 = fig.add_subplot(133)\n",
    "    \n",
    "    def calculate_coverage(indices, pool_info, param='T', bins=10):\n",
    "        if not indices:\n",
    "            return 0\n",
    "        all_values = [info[param] for info in pool_info]\n",
    "        selected_values = [pool_info[i][param] for i in indices if i < len(pool_info)]\n",
    "        \n",
    "        hist_all, edges = np.histogram(all_values, bins=bins)\n",
    "        hist_selected, _ = np.histogram(selected_values, bins=edges)\n",
    "        \n",
    "        coverage = np.sum(hist_selected > 0) / np.sum(hist_all > 0)\n",
    "        return coverage\n",
    "    \n",
    "    params = ['T', 'P', 'ratio']\n",
    "    param_names = ['Temperature', 'Pressure', 'Ratio']\n",
    "    \n",
    "    al_coverage = [calculate_coverage(al_indices, pool_info, p) for p in params]\n",
    "    rand_coverage = [calculate_coverage(random_indices, pool_info, p) for p in params]\n",
    "    \n",
    "    x = np.arange(len(params))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax3.bar(x - width/2, al_coverage, width, label='Active Learning', color='blue', alpha=0.7)\n",
    "    ax3.bar(x + width/2, rand_coverage, width, label='Random Sampling', color='green', alpha=0.7)\n",
    "    \n",
    "    ax3.set_xlabel('Parameter')\n",
    "    ax3.set_ylabel('Coverage')\n",
    "    ax3.set_title('Parameter Space Coverage Comparison')\n",
    "    ax3.set_xticks(x)\n",
    "    ax3.set_xticklabels(param_names)\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add percentage labels on bars\n",
    "    for i, (al_cov, rand_cov) in enumerate(zip(al_coverage, rand_coverage)):\n",
    "        ax3.text(i - width/2, al_cov + 0.01, f'{al_cov:.1%}', ha='center', va='bottom')\n",
    "        ax3.text(i + width/2, rand_cov + 0.01, f'{rand_cov:.1%}', ha='center', va='bottom')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def visualize_mse_comparison(al_mse_history, random_summary):\n",
    "    \"\"\"Direct comparison of Active Learning vs Random Sampling MSE\"\"\"\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "    \n",
    "    # Box plot comparison\n",
    "    ax = axes[0]\n",
    "    \n",
    "    if al_mse_history and random_summary:\n",
    "        # Get final AL MSE\n",
    "        final_al_mse = al_mse_history[-1]['test_mse'] if al_mse_history else None\n",
    "        final_al_size = al_mse_history[-1]['train_size'] if al_mse_history else None\n",
    "        \n",
    "        # Random sampling MSE values\n",
    "        random_mse_values = random_summary.get('all_mse_values', [])\n",
    "        \n",
    "        if final_al_mse and random_mse_values:\n",
    "            data_to_plot = [random_mse_values, [final_al_mse]]\n",
    "            labels = [f'Random Sampling\\n(n={random_summary[\"train_size\"]})', \n",
    "                     f'Active Learning\\n(n={final_al_size})']\n",
    "            \n",
    "            bp = ax.boxplot(data_to_plot, labels=labels, patch_artist=True)\n",
    "            bp['boxes'][0].set_facecolor('lightgreen')\n",
    "            bp['boxes'][1].set_facecolor('lightblue')\n",
    "            \n",
    "            ax.set_ylabel('Test MSE')\n",
    "            ax.set_title('MSE Comparison: Random Sampling vs Active Learning')\n",
    "            ax.grid(True, alpha=0.3, axis='y')\n",
    "            \n",
    "            # Add mean values\n",
    "            ax.plot(1, np.mean(random_mse_values), 'r*', markersize=15, label='Mean')\n",
    "            ax.plot(2, final_al_mse, 'r*', markersize=15)\n",
    "            \n",
    "    # Bar chart comparison\n",
    "    ax = axes[1]\n",
    "    \n",
    "    if al_mse_history and random_summary:\n",
    "        categories = ['Active Learning', 'Random Sampling']\n",
    "        mse_means = [final_al_mse, random_summary['mean_mse']]\n",
    "        mse_stds = [0, random_summary['std_mse']]  # AL has no std as it's single run\n",
    "        \n",
    "        x_pos = np.arange(len(categories))\n",
    "        bars = ax.bar(x_pos, mse_means, yerr=mse_stds, capsize=10, \n",
    "                      color=['blue', 'green'], alpha=0.7)\n",
    "        \n",
    "        ax.set_xlabel('Method')\n",
    "        ax.set_ylabel('Test MSE')\n",
    "        ax.set_title('Average Test MSE Comparison')\n",
    "        ax.set_xticks(x_pos)\n",
    "        ax.set_xticklabels(categories)\n",
    "        ax.grid(True, alpha=0.3, axis='y')\n",
    "        \n",
    "        # Add value labels on bars\n",
    "        for i, (bar, mean_val, std_val) in enumerate(zip(bars, mse_means, mse_stds)):\n",
    "            height = bar.get_height()\n",
    "            if std_val > 0:\n",
    "                label = f'{mean_val:.4f}\\n{std_val:.4f}'\n",
    "            else:\n",
    "                label = f'{mean_val:.4f}'\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + std_val,\n",
    "                   label, ha='center', va='bottom')\n",
    "        \n",
    "        # Add improvement percentage\n",
    "        if random_summary['mean_mse'] > final_al_mse:\n",
    "            improvement = (random_summary['mean_mse'] - final_al_mse) / random_summary['mean_mse'] * 100\n",
    "            ax.text(0.5, max(mse_means) * 0.9, \n",
    "                   f'Active Learning improves by {improvement:.1f}%',\n",
    "                   ha='center', fontsize=12, color='darkred', fontweight='bold')\n",
    "        else:\n",
    "            degradation = (final_al_mse - random_summary['mean_mse']) / random_summary['mean_mse'] * 100\n",
    "            ax.text(0.5, max(mse_means) * 0.9,\n",
    "                   f'Random Sampling improves by {degradation:.1f}%',\n",
    "                   ha='center', fontsize=12, color='darkgreen', fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62897161-6680-4bc5-b55a-ed270e000ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "al_dir = 'AL_results_20250819_162332'\n",
    "random_dir = 'Random_Sampling_Results_20250819_153446'\n",
    "save_dir = 'visualization_results_2'\n",
    "# Create save directory\n",
    "save_dir = os.path.join(BASE_DIR, save_dir)\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Load pool information\n",
    "pool_info = load_pool_info()\n",
    "print(f\"Loaded {len(pool_info)} pool points\")\n",
    "\n",
    "# Visualize entire pool distribution\n",
    "fig = visualize_3d_distribution(pool_info, title=\"Parameter Pool Distribution\")\n",
    "fig.savefig(os.path.join(save_dir, 'pool_distribution_3d.png'), dpi=150)\n",
    "\n",
    "fig = visualize_2d_projections(pool_info, title_prefix=\"Pool\")\n",
    "fig.savefig(os.path.join(save_dir, 'pool_distribution_2d.png'), dpi=150)\n",
    "\n",
    "# Initialize variables for comparison\n",
    "al_indices = []\n",
    "al_mse_history = []\n",
    "train_indices_history = []\n",
    "random_indices = []\n",
    "random_summary = None\n",
    "\n",
    "# If active learning results directory is provided\n",
    "if al_dir:\n",
    "    al_results_dir = os.path.join(BASE_DIR, al_dir)\n",
    "    if os.path.exists(al_results_dir):\n",
    "        al_indices, train_indices_history, al_mse_history = load_active_learning_results(al_results_dir)\n",
    "        print(f\"Active Learning selected {len(al_indices)} points\")\n",
    "        print(f\"Found {len(al_mse_history)} iterations of MSE history\")\n",
    "        \n",
    "        fig = visualize_3d_distribution(pool_info, al_indices, \"Active Learning Sampling Distribution\")\n",
    "        fig.savefig(os.path.join(save_dir, 'active_learning_3d.png'), dpi=150)\n",
    "        \n",
    "        fig = visualize_2d_projections(pool_info, al_indices, \"Active Learning\")\n",
    "        fig.savefig(os.path.join(save_dir, 'active_learning_2d.png'), dpi=150)\n",
    "        \n",
    "        fig = visualize_distribution_statistics(pool_info, al_indices)\n",
    "        fig.savefig(os.path.join(save_dir, 'active_learning_stats.png'), dpi=150)\n",
    "        \n",
    "        if al_mse_history:\n",
    "            fig = visualize_mse_evolution(al_mse_history=al_mse_history)\n",
    "            fig.savefig(os.path.join(save_dir, 'active_learning_mse_evolution.png'), dpi=150)\n",
    "        \n",
    "        if train_indices_history:\n",
    "            fig = visualize_selection_progression(pool_info, train_indices_history)\n",
    "            if fig:\n",
    "                fig.savefig(os.path.join(save_dir, 'active_learning_progression.png'), dpi=150)\n",
    "\n",
    "# If random sampling results directory is provided\n",
    "if random_dir:\n",
    "    random_results_dir = os.path.join(BASE_DIR, random_dir)\n",
    "    if os.path.exists(random_results_dir):\n",
    "        random_indices, random_summary = load_random_sampling_results(random_results_dir)\n",
    "        \n",
    "        if random_indices:\n",
    "            print(f\"Random Sampling selected {len(random_indices)} points\")\n",
    "            \n",
    "            fig = visualize_3d_distribution(pool_info, random_indices, \"Random Sampling Distribution\")\n",
    "            fig.savefig(os.path.join(save_dir, 'random_sampling_3d.png'), dpi=150)\n",
    "            \n",
    "            fig = visualize_2d_projections(pool_info, random_indices, \"Random Sampling\")\n",
    "            fig.savefig(os.path.join(save_dir, 'random_sampling_2d.png'), dpi=150)\n",
    "            \n",
    "            fig = visualize_distribution_statistics(pool_info, random_indices)\n",
    "            fig.savefig(os.path.join(save_dir, 'random_sampling_stats.png'), dpi=150)\n",
    "        \n",
    "        if random_summary:\n",
    "            print(f\"Random Sampling: Mean MSE = {random_summary['mean_mse']:.6f}  {random_summary['std_mse']:.6f}\")\n",
    "\n",
    "# If both AL and random results exist, create comparison plots\n",
    "if al_dir and random_dir and al_indices and random_indices:\n",
    "    fig = compare_sampling_methods(pool_info, al_indices, random_indices)\n",
    "    fig.savefig(os.path.join(save_dir, 'sampling_comparison.png'), dpi=150)\n",
    "    \n",
    "    if al_mse_history and random_summary:\n",
    "        fig = visualize_mse_comparison(al_mse_history, random_summary)\n",
    "        fig.savefig(os.path.join(save_dir, 'mse_comparison.png'), dpi=150)\n",
    "        \n",
    "        # Combined MSE evolution plot\n",
    "        fig = visualize_mse_evolution(al_mse_history=al_mse_history, \n",
    "                                    random_summary=random_summary)\n",
    "        fig.savefig(os.path.join(save_dir, 'combined_mse_evolution.png'), dpi=150)\n",
    "    \n",
    "    print(f\"\\nComparison plots saved\")\n",
    "\n",
    "print(f\"\\nAll visualization results saved to: {save_dir}\")\n",
    "    \n",
    "# Print summary statistics\n",
    "if al_mse_history:\n",
    "    print(f\"\\nActive Learning Final Results:\")\n",
    "    print(f\"  Final MSE: {al_mse_history[-1]['test_mse']:.6f}\")\n",
    "    print(f\"  Final training size: {al_mse_history[-1]['train_size']}\")\n",
    "\n",
    "if random_summary:\n",
    "    print(f\"\\nRandom Sampling Results:\")\n",
    "    print(f\"  Mean MSE: {random_summary['mean_mse']:.6f}  {random_summary['std_mse']:.6f}\")\n",
    "    print(f\"  Training size: {random_summary['train_size']}\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d7be72-d985-4fb2-8f04-86dcb6253ac8",
   "metadata": {},
   "source": [
    "# 3.Baseline Model comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5338f076-2f46-410e-8178-1ae30e747735",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 1. Setup & Single TCN Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1062bd3d-2d20-4d5c-bff2-63a8b65f1a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 1: Setup + Data + Utilities + (Updated Metrics) + Train/Eval TCN ===\n",
    "import os, json, glob, math, copy, random, warnings\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.parametrizations import weight_norm\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ----------------- Basic Configuration (Modifiable) -----------------\n",
    "AL_RESULTS_DIR = \"./AL_results_20250917_192217\"  \n",
    "ITER_FOR_TRAIN = 83                       \n",
    "FIG_DPI = 300\n",
    "TABLE_FIGSIZE = (16, 9)\n",
    "SAVE_ROOT = \"./baseline_eval\"\n",
    "\n",
    "# Consistent with Active Learning\n",
    "INPUT_LEN   = 50\n",
    "PRED_LEN    = 150\n",
    "STRIDE      = 10\n",
    "BATCH_SIZE  = 32\n",
    "EPOCHS      = 120\n",
    "LEARNING_RATE = 1e-3\n",
    "HUBER_DELTA = 1.0\n",
    "HIDDEN_CHANNELS_TCN = [64, 64, 64]\n",
    "KERNEL_SIZE_TCN = 4\n",
    "VAL_SPLIT = 0.1\n",
    "\n",
    "# Test Set\n",
    "TEST_PATHS = [\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_2atm_1per15',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_3atm_1per17.5',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_4atm_1per20',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_5atm_1per22.5',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1100K_6atm_1per25',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_2atm_1per17.5',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_3atm_1per20',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_4atm_1per22.5',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_5atm_1per25',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1200K_6atm_1per15',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_2atm_1per20',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_3atm_1per22.5',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_4atm_1per25',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_5atm_1per15',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1300K_6atm_1per17.5',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_2atm_1per22.5',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_3atm_1per25',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_4atm_1per15',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_5atm_1per17.5',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1400K_6atm_1per20',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_2atm_1per25',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_3atm_1per15',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_4atm_1per17.5',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_5atm_1per20',\n",
    "    r'C:/Users/Administrator/Desktop/My research/2_ML_predict_product/AL_GasKit_V2.0/Total_data/1500K_6atm_1per22.5',\n",
    "]\n",
    "\n",
    "# ----------------- Random Seed -----------------\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed); torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "seed_everything(42)\n",
    "\n",
    "# ----------------- Preprocessor -----------------\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "class DataPreprocessor:\n",
    "    def __init__(self, normalization_method='none', smooth_window=1):\n",
    "        self.normalization_method = normalization_method\n",
    "        self.smooth_window = smooth_window\n",
    "        self.scalers = {}\n",
    "        self.fitted = False\n",
    "        \n",
    "    def fit(self, matrices, species_lists):\n",
    "        all_species_data = {}\n",
    "        for mat, species_list in zip(matrices, species_lists):\n",
    "            for i, sp in enumerate(species_list):\n",
    "                all_species_data.setdefault(sp, []).append(mat[i, :])\n",
    "        for sp, data_list in all_species_data.items():\n",
    "            combined = np.concatenate(data_list)\n",
    "            if self.normalization_method == 'species_wise':\n",
    "                mean, std = np.mean(combined), np.std(combined); std = max(std, 1e-8)\n",
    "                self.scalers[sp] = {'mean': mean, 'std': std}\n",
    "            elif self.normalization_method == 'robust_scaling':\n",
    "                median = np.median(combined); q75, q25 = np.percentile(combined, [75, 25])\n",
    "                iqr = max(q75-q25, 1e-8); self.scalers[sp] = {'median': median, 'iqr': iqr}\n",
    "            elif self.normalization_method == 'min_max':\n",
    "                mn, mx = np.min(combined), np.max(combined); rng = max(mx-mn, 1e-8)\n",
    "                self.scalers[sp] = {'min': mn, 'range': rng}\n",
    "        self.fitted = True\n",
    "\n",
    "    def transform(self, mat, species_list):\n",
    "        if self.smooth_window > 1:\n",
    "            mat = uniform_filter1d(mat, size=self.smooth_window, axis=1)\n",
    "        if self.normalization_method == 'log_transform':\n",
    "            return np.log1p(mat)\n",
    "        if self.normalization_method == 'none':\n",
    "            return mat\n",
    "        out = np.zeros_like(mat)\n",
    "        for i, sp in enumerate(species_list):\n",
    "            if sp in self.scalers:\n",
    "                sc = self.scalers[sp]\n",
    "                if self.normalization_method == 'species_wise':\n",
    "                    out[i, :] = (mat[i, :] - sc['mean']) / sc['std']\n",
    "                elif self.normalization_method == 'robust_scaling':\n",
    "                    out[i, :] = (mat[i, :] - sc['median']) / sc['iqr']\n",
    "                elif self.normalization_method == 'min_max':\n",
    "                    out[i, :] = (mat[i, :] - sc['min']) / sc['range']\n",
    "            else:\n",
    "                out[i, :] = mat[i, :]\n",
    "        return out\n",
    "\n",
    "    def inverse_transform(self, mat, species_list):\n",
    "        if self.normalization_method == 'log_transform':\n",
    "            return np.expm1(mat)\n",
    "        if self.normalization_method == 'none':\n",
    "            return mat\n",
    "        out = np.zeros_like(mat)\n",
    "        for i, sp in enumerate(species_list):\n",
    "            if sp in self.scalers:\n",
    "                sc = self.scalers[sp]\n",
    "                if self.normalization_method == 'species_wise':\n",
    "                    out[i, :] = mat[i, :] * sc['std'] + sc['mean']\n",
    "                elif self.normalization_method == 'robust_scaling':\n",
    "                    out[i, :] = mat[i, :] * sc['iqr'] + sc['median']\n",
    "                elif self.normalization_method == 'min_max':\n",
    "                    out[i, :] = mat[i, :] * sc['range'] + sc['min']\n",
    "            else:\n",
    "                out[i, :] = mat[i, :]\n",
    "        return out\n",
    "\n",
    "# ----------------- Huber Loss (Shared for Training & Evaluation) -----------------\n",
    "class RobustLoss(nn.Module):\n",
    "    def __init__(self, delta=1.0):\n",
    "        super().__init__()\n",
    "        self.delta = delta\n",
    "    def forward(self, pred, target):\n",
    "        diff = torch.abs(pred - target)\n",
    "        mask = diff < self.delta\n",
    "        loss = torch.where(mask, 0.5 * diff ** 2, self.delta * (diff - 0.5 * self.delta))\n",
    "        return loss.mean()\n",
    "\n",
    "def numpy_huber(y_true, y_pred, delta=1.0):\n",
    "    diff = np.abs(y_true - y_pred)\n",
    "    loss = np.where(diff < delta, 0.5 * diff**2, delta * (diff - 0.5*delta))\n",
    "    return float(np.mean(loss))\n",
    "\n",
    "# ----------------- Data Alignment & Loading -----------------\n",
    "def load_species_list(dir_path):\n",
    "    for fn in ['species_list.txt', 'species_list_initial.txt']:\n",
    "        p = os.path.join(dir_path, fn)\n",
    "        if os.path.exists(p):\n",
    "            with open(p, 'r') as f:\n",
    "                s = [line.strip() for line in f if line.strip()]\n",
    "                if s: return s\n",
    "    return None\n",
    "\n",
    "def load_matrix(dir_path):\n",
    "    for fn in ['species_time_matrix.npy', 'species_time_matrix_initial.npy']:\n",
    "        p = os.path.join(dir_path, fn)\n",
    "        if os.path.exists(p):\n",
    "            return np.load(p)\n",
    "    return None\n",
    "\n",
    "def align_to_reference(mat_src, species_src, species_ref):\n",
    "    aligned = np.zeros((len(species_ref), mat_src.shape[1]), dtype=mat_src.dtype)\n",
    "    name_to_idx = {sp:i for i, sp in enumerate(species_src)}\n",
    "    for i, sp in enumerate(species_ref):\n",
    "        if sp in name_to_idx:\n",
    "            aligned[i, :] = mat_src[name_to_idx[sp], :]\n",
    "    return aligned\n",
    "\n",
    "def create_windows(matrices):\n",
    "    X_list, Y_list = [], []\n",
    "    for mat in matrices:\n",
    "        S, T = mat.shape\n",
    "        for start in range(0, T - INPUT_LEN - PRED_LEN + 1, STRIDE):\n",
    "            x = mat[:, start: start + INPUT_LEN]\n",
    "            y = mat[:, start + INPUT_LEN: start + INPUT_LEN + PRED_LEN]\n",
    "            X_list.append(x.T)  # (L_in, S)\n",
    "            Y_list.append(y.T)  # (L_pred, S)\n",
    "    if X_list:\n",
    "        return np.stack(X_list), np.stack(Y_list)\n",
    "    return np.array([]), np.array([])\n",
    "\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X, Y):\n",
    "        self.X = torch.from_numpy(X).float()\n",
    "        self.Y = torch.from_numpy(Y).float()\n",
    "    def __len__(self): return self.X.shape[0]\n",
    "    def __getitem__(self, idx): return self.X[idx], self.Y[idx]\n",
    "\n",
    "# ----------------- Get \"First 84 Rounds\" Training Indices from AL Results -----------------\n",
    "def get_train_indices_from_results(results_dir, iter_k=83):\n",
    "    target = os.path.join(results_dir, f\"state_iter_{iter_k}.json\")\n",
    "    if os.path.exists(target):\n",
    "        with open(target, 'r') as f: st = json.load(f)\n",
    "        return sorted(list(set(st['train_indices'])))\n",
    "    cands = sorted(glob.glob(os.path.join(results_dir, \"state_iter_*.json\")))\n",
    "    best = -1; best_file = None\n",
    "    for p in cands:\n",
    "        try:\n",
    "            i = int(os.path.splitext(os.path.basename(p))[0].split('_')[-1])\n",
    "            if i <= iter_k and i > best:\n",
    "                best, best_file = i, p\n",
    "        except: pass\n",
    "    if best_file:\n",
    "        with open(best_file, 'r') as f: st = json.load(f)\n",
    "        return sorted(list(set(st['train_indices'])))\n",
    "    initf = os.path.join(results_dir, \"initial_train_set.json\")\n",
    "    if os.path.exists(initf):\n",
    "        with open(initf, 'r') as f: arr = json.load(f)\n",
    "        return sorted([d['index'] for d in arr if 'index' in d])\n",
    "    raise FileNotFoundError(\"Training indices not found.\")\n",
    "\n",
    "# ----------------- Get pool_info.json and Locate Training Directory -----------------\n",
    "def infer_base_dir_from_results(results_dir):\n",
    "    return os.path.abspath(os.path.join(results_dir, os.pardir))\n",
    "\n",
    "def get_pool_info(base_dir):\n",
    "    p = os.path.join(base_dir, \"pool_info.json\")\n",
    "    if not os.path.exists(p):\n",
    "        raise FileNotFoundError(f\"Not found {p}\")\n",
    "    with open(p, 'r') as f: return json.load(f)\n",
    "\n",
    "# ----------------- Read Preprocessing Configuration -----------------\n",
    "def read_preprocess_config(results_dir):\n",
    "    cfgp = os.path.join(results_dir, \"config.json\")\n",
    "    if os.path.exists(cfgp):\n",
    "        with open(cfgp, 'r') as f: cfg = json.load(f)\n",
    "        method = cfg.get(\"PREPROCESSING_METHOD\", \"none\")\n",
    "        smooth = int(cfg.get(\"SMOOTH_WINDOW\", 1))\n",
    "        return method, smooth\n",
    "    return \"none\", 1\n",
    "\n",
    "# ----------------- Metrics Function: WAPE & Table -----------------\n",
    "def wape_percent(y_true, y_pred, eps=1e-8):\n",
    "    y = np.asarray(y_true).reshape(-1)\n",
    "    p = np.asarray(y_pred).reshape(-1)\n",
    "    denom = np.sum(np.abs(y))\n",
    "    if denom < eps: return 0.0\n",
    "    return float(100.0 * np.sum(np.abs(y - p)) / denom)\n",
    "\n",
    "def segment_indices(total_steps, short_end=200, long_end=500):\n",
    "    # Returns (s0, s1), (l0, l1) segments clipped within [0, total_steps)\n",
    "    s0, s1 = 0, min(short_end, total_steps)\n",
    "    l0, l1 = 200, min(long_end, total_steps)\n",
    "    if l0 >= l1:  # No valid long segment\n",
    "        l0, l1 = 0, 0\n",
    "    return (s0, s1), (l0, l1)\n",
    "\n",
    "def table_figure(df, save_dir, fname=\"performance_table\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    plt.rcParams['font.family'] = 'sans-serif'\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans', 'Nimbus Sans L', 'Helvetica']\n",
    "    plt.rcParams['font.size'] = 10\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=TABLE_FIGSIZE, dpi=FIG_DPI)\n",
    "    ax.axis('tight'); ax.axis('off')\n",
    "\n",
    "    #  Widen first column: Allocate by axis width ratio (sum  1)\n",
    "    ncol = len(df.columns)\n",
    "    FIRST_COL_W = 0.15  # < Increase to 0.30/0.35 if wider is needed\n",
    "    other_w = (1.0 - FIRST_COL_W) / (ncol - 1)\n",
    "    col_widths = [FIRST_COL_W] + [other_w] * (ncol - 1)\n",
    "\n",
    "    table = ax.table(\n",
    "        cellText=df.values,\n",
    "        colLabels=df.columns,\n",
    "        cellLoc='center',\n",
    "        loc='center',\n",
    "        colWidths=col_widths,   \n",
    "    )\n",
    "\n",
    "    table.auto_set_font_size(False); table.set_fontsize(9); table.scale(1.2, 1.5)\n",
    "\n",
    "    # Allow all cells to wrap text\n",
    "    for (r, c), cell in table.get_celld().items():\n",
    "        cell.get_text().set_wrap(True)\n",
    "\n",
    "    # Left-align the first column (including header), more like a list\n",
    "    table[(0, 0)].set_text_props(ha='left', color='white', weight='bold')\n",
    "    for r in range(1, len(df) + 1):  # Data rows start from 1\n",
    "        table[(r, 0)].set_text_props(ha='left')\n",
    "\n",
    "    # Header and Stripes\n",
    "    for i in range(ncol):\n",
    "        table[(0, i)].set_facecolor('#4CAF50'); table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "    last = len(df)\n",
    "    for i in range(ncol):\n",
    "        table[(last, i)].set_facecolor('#FFF9C4'); table[(last, i)].set_text_props(weight='bold')\n",
    "    for i in range(1, last):\n",
    "        for j in range(ncol):\n",
    "            if i % 2 == 0:\n",
    "                table[(i, j)].set_facecolor('#F5F5F5')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(save_dir, f\"{fname}.png\"), dpi=FIG_DPI, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    plt.close(fig)\n",
    "\n",
    "\n",
    "# ----------------- DTW (Numba optimized, fallback to pure Python on failure) -----------------\n",
    "try:\n",
    "    from numba import njit\n",
    "    @njit\n",
    "    def dtw_distance_numba(a, b):\n",
    "        n, m = a.shape[0], b.shape[0]\n",
    "        DTW = np.full((n+1, m+1), np.inf)\n",
    "        DTW[0,0] = 0.0\n",
    "        for i in range(1, n+1):\n",
    "            for j in range(1, m+1):\n",
    "                cost = abs(a[i-1] - b[j-1])\n",
    "                dp_min = DTW[i-1, j]\n",
    "                if DTW[i, j-1] < dp_min:\n",
    "                    dp_min = DTW[i, j-1]\n",
    "                if DTW[i-1, j-1] < dp_min:\n",
    "                    dp_min = DTW[i-1, j-1]\n",
    "                DTW[i, j] = cost + dp_min\n",
    "        return DTW[n, m]\n",
    "    _ = dtw_distance_numba(np.array([0.], dtype=np.float64), np.array([0.], dtype=np.float64))\n",
    "    DTW_IMPL = \"numba\"\n",
    "except Exception as e:\n",
    "    print(f\"[Info] Numba not available, DTW falling back to pure Python implementation: {e}\")\n",
    "    def dtw_distance_numba(a, b):\n",
    "        n, m = a.shape[0], b.shape[0]\n",
    "        DTW = np.full((n+1, m+1), np.inf)\n",
    "        DTW[0,0] = 0.0\n",
    "        for i in range(1, n+1):\n",
    "            for j in range(1, m+1):\n",
    "                cost = abs(a[i-1] - b[j-1])\n",
    "                DTW[i, j] = cost + min(DTW[i-1, j], DTW[i, j-1], DTW[i-1, j-1])\n",
    "        return DTW[n, m]\n",
    "    DTW_IMPL = \"python\"\n",
    "\n",
    "def average_dtw_across_species(gt_steps_S, pred_steps_S, max_len=200):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "      gt_steps_S: (steps, S)\n",
    "      pred_steps_S: (steps, S)\n",
    "    Returns:\n",
    "      Average DTW (Calculate DTW for each species, then average)\n",
    "    \"\"\"\n",
    "    gt_S = gt_steps_S.T   # -> (S, steps)\n",
    "    pd_S = pred_steps_S.T\n",
    "    S = gt_S.shape[0]\n",
    "    vals = []\n",
    "    for i in range(S):\n",
    "        a = gt_S[i]; b = pd_S[i]\n",
    "        step = max(1, a.shape[0] // max_len)\n",
    "        a_ds = a[::step].astype(np.float64, copy=False)\n",
    "        b_ds = b[::step].astype(np.float64, copy=False)\n",
    "        vals.append(dtw_distance_numba(a_ds, b_ds))\n",
    "    return float(np.mean(vals)) if vals else float('nan')\n",
    "\n",
    "# ----------------- Rolling Forecast Evaluation (New Summary Metrics) -----------------\n",
    "def rolling_forecast(model, initial_input, total_steps, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    steps_done = 0\n",
    "    cur = initial_input.copy()  # (L_in, S)\n",
    "    while steps_done < total_steps:\n",
    "        X = torch.from_numpy(cur[np.newaxis, :, :]).float().to(device)\n",
    "        with torch.no_grad():\n",
    "            out = model(X).detach().cpu().numpy()[0]  # (PRED_LEN, S)\n",
    "        use = min(PRED_LEN, total_steps - steps_done)\n",
    "        preds.append(out[:use])\n",
    "        steps_done += use\n",
    "        if steps_done < total_steps:\n",
    "            sofar = np.vstack(preds)\n",
    "            if sofar.shape[0] >= INPUT_LEN:\n",
    "                cur = sofar[-INPUT_LEN:]\n",
    "            else:\n",
    "                need = INPUT_LEN - sofar.shape[0]\n",
    "                cur = np.vstack([initial_input[-need:], sofar])\n",
    "    return np.vstack(preds)  # (steps, S)\n",
    "\n",
    "def evaluate_model_on_tests(model, species_ref, test_dirs, preproc, save_dir, device):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    rows_summary = []\n",
    "    for td in test_dirs:\n",
    "        mat = load_matrix(td)\n",
    "        sp  = load_species_list(td)\n",
    "        if mat is None or sp is None:\n",
    "            print(f\"[Skip] Matrix/Species file not found: {td}\")\n",
    "            continue\n",
    "        mat_al = align_to_reference(mat, sp, species_ref)  # (S_ref, T)\n",
    "        mat_tr = preproc.transform(mat_al, species_ref)\n",
    "\n",
    "        T = mat_tr.shape[1]\n",
    "        if T <= INPUT_LEN:\n",
    "            print(f\"[Skip] {os.path.basename(td)} T={T} <= INPUT_LEN\")\n",
    "            continue\n",
    "\n",
    "        initial_input = mat_tr[:, :INPUT_LEN].T  # (L_in, S)\n",
    "        total_steps = min(500 - INPUT_LEN, T - INPUT_LEN)  # 450 or smaller\n",
    "        pred = rolling_forecast(model, initial_input, total_steps, device)   # (steps, S)\n",
    "        gt   = mat_tr[:, INPUT_LEN:INPUT_LEN+total_steps].T                 # (steps, S)\n",
    "\n",
    "        # ----- Global Basic Metrics -----\n",
    "        mse  = mean_squared_error(gt.reshape(-1), pred.reshape(-1))\n",
    "        rmse = math.sqrt(mse)\n",
    "        mae  = mean_absolute_error(gt.reshape(-1), pred.reshape(-1))\n",
    "        r2   = r2_score(gt.reshape(-1), pred.reshape(-1))\n",
    "        wape = wape_percent(gt, pred)\n",
    "        huber = numpy_huber(gt, pred, delta=HUBER_DELTA)\n",
    "        pear = float(np.corrcoef(gt.reshape(-1), pred.reshape(-1))[0,1])\n",
    "\n",
    "        # ----- DTW (DTW per species, then average) -----\n",
    "        dtw_avg = average_dtw_across_species(gt, pred, max_len=200)\n",
    "\n",
    "        # ----- Segmentation (short:0-200, long:200-500; auto-clip to total_steps) -----\n",
    "        (s0, s1), (l0, l1) = segment_indices(total_steps, short_end=200, long_end=500)\n",
    "\n",
    "        # short segment\n",
    "        if s1 > s0:\n",
    "            gt_s, pd_s = gt[s0:s1], pred[s0:s1]\n",
    "            short_mse = mean_squared_error(gt_s.reshape(-1), pd_s.reshape(-1))\n",
    "            short_mae = mean_absolute_error(gt_s.reshape(-1), pd_s.reshape(-1))\n",
    "            short_wape = wape_percent(gt_s, pd_s)\n",
    "        else:\n",
    "            short_mse = short_mae = short_wape = float('nan')\n",
    "\n",
    "        # long segment\n",
    "        if l1 > l0:\n",
    "            gt_l, pd_l = gt[l0:l1], pred[l0:l1]\n",
    "            long_mse = mean_squared_error(gt_l.reshape(-1), pd_l.reshape(-1))\n",
    "            long_mae = mean_absolute_error(gt_l.reshape(-1), pd_l.reshape(-1))\n",
    "            long_wape = wape_percent(gt_l, pd_l)\n",
    "        else:\n",
    "            long_mse = long_mae = long_wape = float('nan')\n",
    "\n",
    "        # Record: in your specified order\n",
    "        rows_summary.append({\n",
    "            \"Test Case\":   os.path.basename(td),\n",
    "            \"Huber\":       f\"{huber:.3e}\",\n",
    "            \"MSE\":         f\"{mse:.3e}\",\n",
    "            \"RMSE\":        f\"{rmse:.3e}\",\n",
    "            \"MAE\":         f\"{mae:.3e}\",\n",
    "            \"WAPE (%)\":    f\"{wape:.1f}\",\n",
    "            \"DTW\":         f\"{dtw_avg:.3e}\",\n",
    "            \"pearson\":     f\"{pear:.3f}\",\n",
    "            \"R2\":          f\"{r2:.3f}\",\n",
    "            \"short_MSE\":   f\"{short_mse:.3e}\" if not np.isnan(short_mse) else \"\",\n",
    "            \"long_MSE\":    f\"{long_mse:.3e}\"  if not np.isnan(long_mse)  else \"\",\n",
    "            \"short_MAE\":   f\"{short_mae:.3e}\" if not np.isnan(short_mae) else \"\",\n",
    "            \"long_MAE\":    f\"{long_mae:.3e}\"  if not np.isnan(long_mae)  else \"\",\n",
    "            \"short_WAPE\":  f\"{short_wape:.1f}\" if not np.isnan(short_wape) else \"\",\n",
    "            \"long_WAPE\":   f\"{long_wape:.1f}\"  if not np.isnan(long_wape)  else \"\",\n",
    "        })\n",
    "\n",
    "        # Optional: save predictions and ground truth\n",
    "        case_tag = os.path.basename(td)\n",
    "        np.save(os.path.join(save_dir, f\"{case_tag}_pred.npy\"), pred)\n",
    "        np.save(os.path.join(save_dir, f\"{case_tag}_gt.npy\"), gt)\n",
    "\n",
    "    if not rows_summary:\n",
    "        raise RuntimeError(\"Failed to evaluate any test sets.\")\n",
    "\n",
    "    # Create DataFrame (ensure column order matches requirements)\n",
    "    cols_order = [\"Test Case\", \"Huber\",  \"short_MSE\", \"long_MSE\", \"MSE\", \"RMSE\", \"short_MAE\", \"long_MAE\", \"MAE\", \"short_WAPE\", \"long_WAPE\", \"WAPE (%)\",\n",
    "                  \"DTW\", \"pearson\", \"R2\"]\n",
    "    df = pd.DataFrame(rows_summary)[cols_order]\n",
    "\n",
    "    # Append average row (format by column type)\n",
    "    avg = {\"Test Case\": \"Average\"}\n",
    "    for col in cols_order[1:]:\n",
    "        vals=[]\n",
    "        for v in df[col]:\n",
    "            try: vals.append(float(str(v)))\n",
    "            except: pass\n",
    "        if not vals:\n",
    "            avg[col] = \"\"\n",
    "        elif col in [\"WAPE (%)\",\"short_WAPE\",\"long_WAPE\"]:\n",
    "            avg[col] = f\"{np.nanmean(vals):.1f}\"\n",
    "        elif col in [\"pearson\",\"R2\"]:\n",
    "            avg[col] = f\"{np.nanmean(vals):.3f}\"\n",
    "        elif col in [\"MSE\",\"RMSE\",\"MAE\",\"Huber\",\"DTW\",\"short_MSE\",\"long_MSE\",\"short_MAE\",\"long_MAE\"]:\n",
    "            avg[col] = f\"{np.nanmean(vals):.3e}\"\n",
    "        else:\n",
    "            avg[col] = f\"{np.nanmean(vals):.3f}\"\n",
    "    df = pd.concat([df, pd.DataFrame([avg])], ignore_index=True)\n",
    "\n",
    "    # Save & Plot\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    df.to_csv(os.path.join(save_dir, \"performance_metrics.csv\"), index=False)\n",
    "    table_figure(df, save_dir, fname=\"performance_table\")\n",
    "    return df\n",
    "\n",
    "# ----------------- Generic Trainer -----------------\n",
    "def train_and_evaluate(model, model_name, X_train, Y_train, species_ref, preproc, device, save_root=SAVE_ROOT):\n",
    "    os.makedirs(save_root, exist_ok=True)\n",
    "    # Split Train/Validation\n",
    "    n = X_train.shape[0]\n",
    "    idx = np.arange(n)\n",
    "    np.random.shuffle(idx)\n",
    "    n_val = max(1, int(n * VAL_SPLIT))\n",
    "    val_idx, tr_idx = idx[:n_val], idx[n_val:]\n",
    "    ds_tr = TimeSeriesDataset(X_train[tr_idx], Y_train[tr_idx])\n",
    "    ds_va = TimeSeriesDataset(X_train[val_idx], Y_train[val_idx])\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    dl_va = DataLoader(ds_va, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    model = model.to(device)\n",
    "    optim = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    criterion = RobustLoss(delta=HUBER_DELTA).to(device)\n",
    "\n",
    "    best_loss = float(\"inf\"); best_state = None; patience=15; bad=0\n",
    "    for ep in range(1, EPOCHS+1):\n",
    "        model.train(); run_loss=0.0\n",
    "        for xb, yb in dl_tr:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            optim.zero_grad()\n",
    "            pred = model(xb)\n",
    "            loss = criterion(pred, yb)\n",
    "            loss.backward(); optim.step()\n",
    "            run_loss += loss.item()\n",
    "        tr_loss = run_loss/len(dl_tr)\n",
    "\n",
    "        # Validation\n",
    "        model.eval(); va_loss=0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, yb in dl_va:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                pred = model(xb)\n",
    "                va_loss += criterion(pred, yb).item()\n",
    "        va_loss /= len(dl_va)\n",
    "\n",
    "        if ep % 20 == 0 or ep==1:\n",
    "            print(f\"[{model_name}] Epoch {ep:03d} | Train {tr_loss:.4f} | Val {va_loss:.4f}\")\n",
    "\n",
    "        if va_loss + 1e-6 < best_loss:\n",
    "            best_loss = va_loss; best_state = copy.deepcopy(model.state_dict()); bad=0\n",
    "        else:\n",
    "            bad += 1\n",
    "            if bad >= patience:\n",
    "                print(f\"[{model_name}] Early stopping at epoch {ep}\")\n",
    "                break\n",
    "\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # Evaluation\n",
    "    save_dir = os.path.join(save_root, model_name)\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    torch.save(model.state_dict(), os.path.join(save_dir, f\"{model_name}.pth\"))\n",
    "    print(f\"[{model_name}]  Training complete, evaluating on test set (new metrics)...\")\n",
    "    df = evaluate_model_on_tests(model, species_ref, TEST_PATHS, preproc, save_dir, device)\n",
    "    print(f\"[{model_name}] Evaluation complete, results saved in: {save_dir}\")\n",
    "    return model, df\n",
    "\n",
    "# ----------------- Build TCN -----------------\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv1d(in_ch, out_ch, kernel_size, stride=stride, padding=padding, dilation=dilation))\n",
    "        self.relu1 = nn.ReLU(); self.drop1 = nn.Dropout(dropout)\n",
    "        self.conv2 = weight_norm(nn.Conv1d(out_ch, out_ch, kernel_size, stride=stride, padding=padding, dilation=dilation))\n",
    "        self.relu2 = nn.ReLU(); self.drop2 = nn.Dropout(dropout)\n",
    "        self.net = nn.Sequential(self.conv1, self.relu1, self.drop1, self.conv2, self.relu2, self.drop2)\n",
    "        self.downsample = nn.Conv1d(in_ch, out_ch, 1) if in_ch != out_ch else None\n",
    "        self.relu = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        if out.size(2) != res.size(2):\n",
    "            out = out[:, :, :res.size(2)]\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=3, dropout=0.2):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        for i in range(len(num_channels)):\n",
    "            dilation = 2 ** i\n",
    "            in_ch  = num_inputs if i==0 else num_channels[i-1]\n",
    "            out_ch = num_channels[i]\n",
    "            padding = (kernel_size - 1) * dilation\n",
    "            layers += [TemporalBlock(in_ch, out_ch, kernel_size, 1, dilation, padding, dropout)]\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    def forward(self, x):  # x: (B, C, L)\n",
    "        return self.network(x)\n",
    "\n",
    "class TCNForecast(nn.Module):\n",
    "    def __init__(self, input_dim, num_channels, kernel_size, pred_len, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.tcn = TemporalConvNet(input_dim, num_channels, kernel_size, dropout)\n",
    "        self.linear = nn.Linear(num_channels[-1], input_dim * pred_len)\n",
    "        self.input_dim = input_dim; self.pred_len = pred_len\n",
    "    def forward(self, x):   # x: (B, L, C)\n",
    "        x = x.transpose(1, 2)       # -> (B, C, L)\n",
    "        y = self.tcn(x)             # -> (B, H, L)\n",
    "        out = y[:, :, -1]           # -> (B, H)\n",
    "        pred = self.linear(out)     # -> (B, C*P)\n",
    "        return pred.view(-1, self.pred_len, self.input_dim)\n",
    "\n",
    "# ----------------- Construct Train/Test Data (First 84 Rounds) -----------------\n",
    "print(\"Preparing train/test data...\")\n",
    "BASE_DIR = infer_base_dir_from_results(AL_RESULTS_DIR)\n",
    "pool_info = get_pool_info(BASE_DIR)\n",
    "train_indices = get_train_indices_from_results(AL_RESULTS_DIR, ITER_FOR_TRAIN)\n",
    "print(f\"Number of training points in first 84 rounds: {len(train_indices)}\")\n",
    "\n",
    "# Reference species\n",
    "first_train_dir = pool_info[train_indices[0]]['sim_dir']\n",
    "species_ref = load_species_list(first_train_dir)\n",
    "if species_ref is None:\n",
    "    raise FileNotFoundError(\"Reference species list not found.\")\n",
    "print(f\"Number of reference species: {len(species_ref)}\")\n",
    "\n",
    "# Read preprocessing configuration\n",
    "def read_preprocess_config(results_dir):\n",
    "    cfgp = os.path.join(results_dir, \"config.json\")\n",
    "    if os.path.exists(cfgp):\n",
    "        with open(cfgp, 'r') as f: cfg = json.load(f)\n",
    "        method = cfg.get(\"PREPROCESSING_METHOD\", \"none\")\n",
    "        smooth = int(cfg.get(\"SMOOTH_WINDOW\", 1))\n",
    "        return method, smooth\n",
    "    return \"none\", 1\n",
    "\n",
    "pp_method, pp_smooth = read_preprocess_config(AL_RESULTS_DIR)\n",
    "print(f\"Preprocessing: method={pp_method}, smooth_window={pp_smooth}\")\n",
    "preproc = DataPreprocessor(normalization_method=pp_method, smooth_window=pp_smooth)\n",
    "\n",
    "# Assemble training matrices\n",
    "train_mats_raw, train_species = [], []\n",
    "for idx in train_indices:\n",
    "    d = pool_info[idx]['sim_dir']\n",
    "    mat = load_matrix(d); sp = load_species_list(d)\n",
    "    if mat is None or sp is None: \n",
    "        continue\n",
    "    al = align_to_reference(mat, sp, species_ref)\n",
    "    train_mats_raw.append(al); train_species.append(species_ref)\n",
    "if not train_mats_raw:\n",
    "    raise RuntimeError(\"Training matrices are empty.\")\n",
    "\n",
    "# Fit preprocessor and transform\n",
    "preproc.fit(train_mats_raw, train_species)\n",
    "train_mats = [preproc.transform(m, species_ref) for m in train_mats_raw]\n",
    "\n",
    "# Create windows\n",
    "X_train, Y_train = create_windows(train_mats)\n",
    "print(f\"Training windows: X={X_train.shape}, Y={Y_train.shape}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ----------------- Train and Evaluate: TCN -----------------\n",
    "tcn = TCNForecast(input_dim=len(species_ref), num_channels=HIDDEN_CHANNELS_TCN, \n",
    "                  kernel_size=KERNEL_SIZE_TCN, pred_len=PRED_LEN, dropout=0.2)\n",
    "_ , df_tcn = train_and_evaluate(tcn, \"TCN\", X_train, Y_train, species_ref, preproc, device, save_root=SAVE_ROOT)\n",
    "df_tcn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ea9659-3cc5-4122-8eea-4998e536422c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 2. SIngle LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045fd37d-df54-4b9a-9496-80027fa90c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMForecast(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, pred_len=150):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc   = nn.Linear(hidden_dim, input_dim * pred_len)\n",
    "        self.input_dim = input_dim; self.pred_len = pred_len\n",
    "    def forward(self, x):  # x: (B, L, C)\n",
    "        out, _ = self.lstm(x)         # (B, L, H)\n",
    "        h_last = out[:, -1, :]        # (B, H)\n",
    "        y = self.fc(h_last)           # (B, C*P)\n",
    "        return y.view(-1, self.pred_len, self.input_dim)\n",
    "\n",
    "lstm = LSTMForecast(input_dim=len(species_ref), hidden_dim=64, num_layers=3, pred_len=PRED_LEN)\n",
    "_, df_lstm = train_and_evaluate(lstm, \"LSTM\", X_train, Y_train, species_ref, preproc, device, save_root=SAVE_ROOT)\n",
    "df_lstm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c75a74-8264-4342-9801-9c4d532cf502",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 3. Single GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c283fa29-66f5-4625-b880-c32698901ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class GRUForecast(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, pred_len=150):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size=input_dim, hidden_size=hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc  = nn.Linear(hidden_dim, input_dim * pred_len)\n",
    "        self.input_dim = input_dim; self.pred_len = pred_len\n",
    "    def forward(self, x):\n",
    "        out, _ = self.gru(x)\n",
    "        h_last = out[:, -1, :]\n",
    "        y = self.fc(h_last)\n",
    "        return y.view(-1, self.pred_len, self.input_dim)\n",
    "\n",
    "gru = GRUForecast(input_dim=len(species_ref), hidden_dim=64, num_layers=3, pred_len=PRED_LEN)\n",
    "_, df_gru = train_and_evaluate(gru, \"GRU\", X_train, Y_train, species_ref, preproc, device, save_root=SAVE_ROOT)\n",
    "df_gru\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "577e7e3c-ce71-4bea-8708-c93e8a7c040c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 4. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d4aea3-3d32-4306-879c-48bf803bacd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNNForecast(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, pred_len=150, nonlinearity='tanh', dropout=0.0):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.RNN(input_size=input_dim, hidden_size=hidden_dim,\n",
    "                          num_layers=num_layers, nonlinearity=nonlinearity,\n",
    "                          batch_first=True, dropout=dropout if num_layers>1 else 0.0)\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim * pred_len)\n",
    "        self.input_dim = input_dim\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def forward(self, x):                 # x: (B, L, C)\n",
    "        out, _ = self.rnn(x)              # -> (B, L, H)\n",
    "        h_last = out[:, -1, :]            # -> (B, H)\n",
    "        y = self.fc(h_last)               # -> (B, C*P)\n",
    "        return y.view(-1, self.pred_len, self.input_dim)\n",
    "\n",
    "rnn_tanh = RNNForecast(input_dim=len(species_ref), hidden_dim=64, num_layers=3, pred_len=PRED_LEN, nonlinearity='tanh', dropout=0.0)\n",
    "_, df_rnn = train_and_evaluate(rnn_tanh, \"RNN_Tanh\", X_train, Y_train, species_ref, preproc, device, save_root=SAVE_ROOT)\n",
    "df_rnn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7112b07a-7465-4499-9099-8e1626f96b74",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. BiLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdb94bc-6bd4-4f09-98e5-709488a0e0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMForecast(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, num_layers=2, pred_len=150, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.bilstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_dim,\n",
    "                              num_layers=num_layers, batch_first=True,\n",
    "                              bidirectional=True, dropout=dropout if num_layers>1 else 0.0)\n",
    "        self.fc = nn.Linear(2 * hidden_dim, input_dim * pred_len)\n",
    "        self.input_dim = input_dim\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def forward(self, x):                 # x: (B, L, C)\n",
    "        out, _ = self.bilstm(x)           # -> (B, L, 2H)\n",
    "        h_last = out[:, -1, :]            # -> (B, 2H)\n",
    "        y = self.fc(h_last)               # -> (B, C*P)\n",
    "        return y.view(-1, self.pred_len, self.input_dim)\n",
    "\n",
    "bilstm = BiLSTMForecast(input_dim=len(species_ref), hidden_dim=64, num_layers=2, pred_len=PRED_LEN, dropout=0.1)\n",
    "_, df_bilstm = train_and_evaluate(bilstm, \"BiLSTM\", X_train, Y_train, species_ref, preproc, device, save_root=SAVE_ROOT)\n",
    "df_bilstm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cddd6c-376d-49d2-bb0a-d69aac446a5a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 6. Transformer Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bd566e-2d32-4025-848e-d88b23abf834",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=2048):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pos = torch.arange(0, max_len, dtype=torch.float32).unsqueeze(1)\n",
    "        div = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float32) * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(pos * div)\n",
    "        pe[:, 1::2] = torch.cos(pos * div)\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))  # (1, L, d)\n",
    "    def forward(self, x):  # x: (B, L, d)\n",
    "        return x + self.pe[:, :x.size(1), :]\n",
    "\n",
    "class TransformerForecast(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=128, nhead=8, num_layers=3, dim_feedforward=256, pred_len=150, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.inp_proj = nn.Linear(input_dim, d_model)\n",
    "        enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(enc_layer, num_layers=num_layers)\n",
    "        self.pos = PositionalEncoding(d_model)\n",
    "        self.fc = nn.Linear(d_model, input_dim * pred_len)\n",
    "        self.input_dim = input_dim; self.pred_len = pred_len\n",
    "    def forward(self, x):   # x: (B, L, C)\n",
    "        z = self.inp_proj(x)\n",
    "        z = self.pos(z)\n",
    "        z = self.encoder(z)            # (B, L, d)\n",
    "        h = z[:, -1, :]                # (B, d)\n",
    "        y = self.fc(h)                 # (B, C*P)\n",
    "        return y.view(-1, self.pred_len, self.input_dim)\n",
    "\n",
    "tfm = TransformerForecast(input_dim=len(species_ref), d_model=64, nhead=8, num_layers=3, dim_feedforward=128, pred_len=PRED_LEN, dropout=0.1)\n",
    "_, df_tfm = train_and_evaluate(tfm, \"Transformer\", X_train, Y_train, species_ref, preproc, device, save_root=SAVE_ROOT)\n",
    "df_tfm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2a89f7-a041-43e1-99fd-d362e33575a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 7.CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187779be-d0bf-4ceb-9d67-659c733394d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNLSTMForecast(nn.Module):\n",
    "    def __init__(self, input_dim, conv_channels=128, kernel_size=5,\n",
    "                 lstm_hidden=128, lstm_layers=1, pred_len=150, dropout=0.1):\n",
    "        super().__init__()\n",
    "        padding = kernel_size // 2\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv1d(input_dim, conv_channels, kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(conv_channels, conv_channels, kernel_size, padding=padding),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.lstm = nn.LSTM(input_size=conv_channels, hidden_size=lstm_hidden,\n",
    "                            num_layers=lstm_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(lstm_hidden, input_dim * pred_len)\n",
    "        self.input_dim = input_dim\n",
    "        self.pred_len = pred_len\n",
    "\n",
    "    def forward(self, x):                 # x: (B, L, C)\n",
    "        z = x.transpose(1, 2)             # -> (B, C, L)\n",
    "        z = self.conv(z)                  # -> (B, conv_channels, L)\n",
    "        z = z.transpose(1, 2)             # -> (B, L, conv_channels)\n",
    "        enc, _ = self.lstm(z)             # -> (B, L, H)\n",
    "        h_last = enc[:, -1, :]            # -> (B, H)\n",
    "        y = self.fc(h_last)               # -> (B, C*P)\n",
    "        return y.view(-1, self.pred_len, self.input_dim)\n",
    "\n",
    "cnnlstm = CNNLSTMForecast(\n",
    "    input_dim=len(species_ref), conv_channels=128, kernel_size=5,\n",
    "    lstm_hidden=128, lstm_layers=1, pred_len=PRED_LEN, dropout=0.1\n",
    ")\n",
    "_, df_cnnlstm = train_and_evaluate(cnnlstm, \"CNN_LSTM\", X_train, Y_train, species_ref, preproc, device, save_root=SAVE_ROOT)\n",
    "df_cnnlstm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44b4d1a9-7657-4eed-89d6-f13b95c65b00",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 8. Seq2Seq LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d61268-dda2-4091-890f-452345cbb621",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class Seq2SeqLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hid=128, layers=2, pred_len=150):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.LSTM(input_size=input_dim, hidden_size=hid, num_layers=layers, batch_first=True)\n",
    "        self.decoder_cell = nn.LSTMCell(input_size=input_dim, hidden_size=hid)\n",
    "        self.proj = nn.Linear(hid, input_dim)\n",
    "        self.pred_len = pred_len; self.input_dim = input_dim\n",
    "    def forward(self, x):  # x: (B,L,C)\n",
    "        B = x.size(0)\n",
    "        enc_out, (h, c) = self.encoder(x)   # h,c: (layers,B,H)\n",
    "        h_t, c_t = h[-1], c[-1]             # \n",
    "        y_t = x[:, -1, :]                   # \n",
    "        outs = []\n",
    "        for _ in range(self.pred_len):\n",
    "            h_t, c_t = self.decoder_cell(y_t, (h_t, c_t))\n",
    "            step = self.proj(h_t)           # (B,C)\n",
    "            outs.append(step.unsqueeze(1))\n",
    "            y_t = step\n",
    "        return torch.cat(outs, dim=1)       # (B,P,C)\n",
    "\n",
    "s2s = Seq2SeqLSTM(input_dim=len(species_ref), hid=64, layers=3, pred_len=PRED_LEN)\n",
    "_, df_s2s = train_and_evaluate(s2s, \"Seq2Seq_LSTM\", X_train, Y_train, species_ref, preproc, device, save_root=SAVE_ROOT)\n",
    "df_s2s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cd5610-2640-458c-8aa7-c50ae279847a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 9. ResMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c392aa39-87ba-4478-9c8d-ff14d6bad845",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, d, p=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(d, d),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p),\n",
    "            nn.Linear(d, d)\n",
    "        )\n",
    "        self.act = nn.ReLU()\n",
    "    def forward(self, x):\n",
    "        return self.act(x + self.net(x))\n",
    "\n",
    "class ResMLP(nn.Module):\n",
    "    def __init__(self, input_dim, pred_len=150, width=512, depth=3, p=0.1):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim; self.pred_len = pred_len\n",
    "        d_in  = INPUT_LEN * input_dim\n",
    "        d_out = pred_len * input_dim\n",
    "        self.stem = nn.Sequential(nn.Linear(d_in, width), nn.ReLU(), nn.Dropout(p))\n",
    "        self.blocks = nn.Sequential(*[ResBlock(width, p=p) for _ in range(depth)])\n",
    "        self.head = nn.Linear(width, d_out)\n",
    "    def forward(self, x):          # x: (B,L,C)\n",
    "        z = x.reshape(x.size(0), -1)\n",
    "        z = self.stem(z)\n",
    "        z = self.blocks(z)\n",
    "        y = self.head(z)\n",
    "        return y.view(-1, self.pred_len, self.input_dim)\n",
    "\n",
    "resmlp = ResMLP(input_dim=len(species_ref), pred_len=PRED_LEN, width=256, depth=3, p=0.1)\n",
    "_, df_resmlp = train_and_evaluate(resmlp, \"ResMLP\", X_train, Y_train, species_ref, preproc, device, save_root=SAVE_ROOT)\n",
    "df_resmlp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4bd67a-2f65-4ec4-a6f8-90a48911530d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Scatter Plot (RNN) =======================\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, os, re\n",
    "from matplotlib.cm import get_cmap\n",
    "\n",
    "VIZ_DIR   = \"./baseline_eval/_viz\"\n",
    "TCN_NAME  = \"TCN\"\n",
    "\n",
    "# --------  --------\n",
    "STYLE = {\n",
    "    \"figsize\": (10, 8),        # \n",
    "    \"label_fontsize\": 20,      # \n",
    "    \"tick_fontsize\": 16,       # \n",
    "    \"legend_fontsize\": 16,     # \n",
    "    \"spine_linewidth\": 2.5,\n",
    "    \"spine_color\": \"black\",\n",
    "    \"tick_length\": 4.0,\n",
    "    \"tick_width\": 1.3,\n",
    "    \"axis_labelpad\": 6,        # \n",
    "    \"grid\": True,              # \n",
    "    \"dpi\": 300,                #  dpi\n",
    "    \"legend_frameon\": False,   # \n",
    "    \"tcn_color\": \"tab:blue\",   # TCN \n",
    "    \"baseline_marker\": \"o\",    # \n",
    "    \"tcn_marker\": \"D\",         # TCN \n",
    "    \"alpha\": 0.75,             # \n",
    "    \"tcn_size\": 90,            # TCN \n",
    "    \"baseline_size\": 52,       # \n",
    "}\n",
    "\n",
    "# ****\n",
    "LEGEND_ORDER_CANONICAL = [\n",
    "    \"TCN\", \"RNN\", \"LSTM\", \"BiLSTM\", \"CNN_LSTM\", \"Seq2Seq_LSTM\", \"GRU\", \"Transformer\", \"ResMLP\"\n",
    "]\n",
    "\n",
    "# //\n",
    "ALIASES_LOWER = {\n",
    "    \"tcn\": \"TCN\",\n",
    "    \"rnn\": \"RNN\", \"rnn_tanh\": \"RNN\", \"rnn-tanh\": \"RNN\", \"rnn tanh\": \"RNN\", \"vanilla rnn\": \"RNN\",\n",
    "    \"lstm\": \"LSTM\",\n",
    "    \"bilstm\": \"BiLSTM\", \"bi-lstm\": \"BiLSTM\", \"bi lstm\": \"BiLSTM\", \"bi_lstm\": \"BiLSTM\",\n",
    "    \"cnn_lstm\": \"CNN_LSTM\", \"cnn-lstm\": \"CNN_LSTM\", \"cnn lstm\": \"CNN_LSTM\",\n",
    "    \"seq2seq_lstm\": \"Seq2Seq_LSTM\", \"seq2seq-lstm\": \"Seq2Seq_LSTM\", \"seq2seq lstm\": \"Seq2Seq_LSTM\",\n",
    "    \"gru\": \"GRU\",\n",
    "    \"transformer\": \"Transformer\",\n",
    "    \"resmlp\": \"ResMLP\",\n",
    "}\n",
    "\n",
    "def to_canonical(name: str) -> str:\n",
    "    \"\"\"_- RNN_tanh  RNN\"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return str(name)\n",
    "    s = name.strip().lower()\n",
    "    s = re.sub(r\"[^\\w\\s\\-]+\", \"\", s)           # \n",
    "    s = s.replace(\"__\",\"_\").replace(\"-\", \" \")  # \n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    # \n",
    "    if s in ALIASES_LOWER:\n",
    "        return ALIASES_LOWER[s]\n",
    "    # \n",
    "    s2 = s.replace(\" \", \"_\")\n",
    "    if s2 in ALIASES_LOWER:\n",
    "        return ALIASES_LOWER[s2]\n",
    "    # \n",
    "    return name.strip()\n",
    "\n",
    "def build_color_map(canonical_list, tcn_color=\"tab:blue\", cmap_name=\"tab20\"):\n",
    "    \"\"\"TCN\"\"\"\n",
    "    cmap = get_cmap(cmap_name)\n",
    "    palette = [cmap(i % cmap.N) for i in range(max(20, len(canonical_list)))]\n",
    "    color_map = {}\n",
    "    #  TCN \n",
    "    if \"TCN\" in canonical_list:\n",
    "        color_map[\"TCN\"] = tcn_color\n",
    "    idx = 0\n",
    "    for cn in canonical_list:\n",
    "        if cn in color_map:\n",
    "            continue\n",
    "        color_map[cn] = palette[idx]\n",
    "        idx += 1\n",
    "    return color_map\n",
    "\n",
    "df = pd.read_csv(os.path.join(VIZ_DIR, \"all_models_by_test.csv\"))\n",
    "\n",
    "def scatter_xy(xm, ym, xlog=True, ylog=False, save_name=\"scatter.png\"):\n",
    "    fig, ax = plt.subplots(figsize=STYLE[\"figsize\"], dpi=STYLE[\"dpi\"])\n",
    "\n",
    "    # 1)  RNN_tanh  RNN\n",
    "    df_ = df[[\"Model\", xm, ym]].dropna().copy()\n",
    "    df_[\"Canon\"] = df_[\"Model\"].apply(to_canonical)\n",
    "\n",
    "    # 2) \n",
    "    canon_present = []\n",
    "    for k in LEGEND_ORDER_CANONICAL:\n",
    "        if (df_[\"Canon\"] == k).any():\n",
    "            canon_present.append(k)\n",
    "    # \n",
    "    others = sorted(set(df_[\"Canon\"].unique()) - set(canon_present))\n",
    "    canon_present += others\n",
    "\n",
    "    # 3) \n",
    "    color_map = build_color_map(canon_present, tcn_color=STYLE[\"tcn_color\"])\n",
    "\n",
    "    # 4) \n",
    "    handles, labels = [], []\n",
    "    for cn in canon_present:\n",
    "        sub = df_.loc[df_[\"Canon\"] == cn, [xm, ym]]\n",
    "        if sub.empty:\n",
    "            continue\n",
    "        is_tcn = (cn == \"TCN\")\n",
    "        sc = ax.scatter(\n",
    "            sub[xm], sub[ym],\n",
    "            s=STYLE[\"tcn_size\"] if is_tcn else STYLE[\"baseline_size\"],\n",
    "            alpha=STYLE[\"alpha\"],\n",
    "            label=cn,\n",
    "            marker=STYLE[\"tcn_marker\"] if is_tcn else STYLE[\"baseline_marker\"],\n",
    "            color=color_map.get(cn, \"tab:gray\"),\n",
    "            edgecolor=\"none\",\n",
    "        )\n",
    "        handles.append(sc); labels.append(cn)\n",
    "\n",
    "    # 5) \n",
    "    ax.set_xlabel(xm, fontsize=STYLE[\"label_fontsize\"], labelpad=STYLE[\"axis_labelpad\"])\n",
    "    ax.set_ylabel(ym, fontsize=STYLE[\"label_fontsize\"], labelpad=STYLE[\"axis_labelpad\"])\n",
    "    if xlog: ax.set_xscale(\"log\")\n",
    "    if ylog: ax.set_yscale(\"log\")\n",
    "\n",
    "    # \n",
    "    ax.tick_params(axis='both', which='both',\n",
    "                   labelsize=STYLE[\"tick_fontsize\"],\n",
    "                   length=STYLE[\"tick_length\"],\n",
    "                   width=STYLE[\"tick_width\"])\n",
    "    for spine in [\"top\",\"right\",\"left\",\"bottom\"]:\n",
    "        ax.spines[spine].set_linewidth(STYLE[\"spine_linewidth\"])\n",
    "        ax.spines[spine].set_color(STYLE[\"spine_color\"])\n",
    "\n",
    "    # \n",
    "    if STYLE[\"grid\"]:\n",
    "        ax.grid(ls=\"--\", alpha=0.3)\n",
    "\n",
    "    # 6) \n",
    "    lh = {lab: h for lab, h in zip(labels, handles)}\n",
    "    legend_order = [lab for lab in LEGEND_ORDER_CANONICAL if lab in lh] + \\\n",
    "                   [lab for lab in labels if lab not in LEGEND_ORDER_CANONICAL]\n",
    "    legend_handles = [lh[lab] for lab in legend_order]\n",
    "\n",
    "    ax.legend(\n",
    "        legend_handles, legend_order,\n",
    "        loc=\"best\",\n",
    "        frameon=STYLE[\"legend_frameon\"],\n",
    "        fontsize=STYLE[\"legend_fontsize\"],\n",
    "        ncol=2\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(VIZ_DIR, save_name), bbox_inches=\"tight\", dpi=STYLE[\"dpi\"])\n",
    "    plt.show()\n",
    "\n",
    "# \n",
    "scatter_xy(\"MSE\", \"DTW\", xlog=True, ylog=False, save_name=\"scatter_MSE_vs_DTW.png\")\n",
    "# scatter_xy(\"Huber\", \"DTW\", xlog=True, ylog=True, save_name=\"scatter_Huber_vs_DTW.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3b3dfa-55bf-48e6-a11b-c40727f649bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell A3: Architectural benchmarking table (averages) =====================\n",
    "import os, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "\n",
    "VIZ_DIR   = \"./baseline_eval/_viz\"\n",
    "AVG_CSV   = os.path.join(VIZ_DIR, \"all_models_averages.csv\")\n",
    "OUT_CSV   = os.path.join(VIZ_DIR, \"TableX_architectural_benchmark.csv\")\n",
    "OUT_PNG   = os.path.join(VIZ_DIR, \"TableX_architectural_benchmark.png\")\n",
    "\n",
    "# \n",
    "METRICS_ORDER = [\"Huber\",\"MSE\",\"RMSE\",\"MAE\",\"WAPE (%)\",\"DTW\",\"pearson\",\"R2\",\n",
    "                 \"short_MSE\",\"long_MSE\",\"short_MAE\",\"long_MAE\",\"short_WAPE\",\"long_WAPE\"]\n",
    "\n",
    "df = pd.read_csv(AVG_CSV)\n",
    "\n",
    "#  Test Case \n",
    "df = df[[\"Model\"] + [c for c in METRICS_ORDER if c in df.columns]].copy()\n",
    "\n",
    "# \n",
    "def fmt(v, col):\n",
    "    try:\n",
    "        x = float(v)\n",
    "    except:\n",
    "        return v\n",
    "    if col in [\"WAPE (%)\",\"short_WAPE\",\"long_WAPE\",\"pearson\",\"R2\"]:\n",
    "        return f\"{x:.3f}\" if col in [\"pearson\",\"R2\"] else f\"{x:.1f}\"\n",
    "    else:\n",
    "        return f\"{x:.3e}\"\n",
    "\n",
    "for c in METRICS_ORDER:\n",
    "    if c in df.columns:\n",
    "        df[c] = df[c].apply(lambda v: fmt(v, c))\n",
    "\n",
    "#  CSV\n",
    "df.to_csv(OUT_CSV, index=False)\n",
    "\n",
    "# Arial \n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = ['Arial', 'DejaVu Sans', 'Liberation Sans', 'Helvetica']\n",
    "fig, ax = plt.subplots(figsize=(16, 9), dpi=300)\n",
    "ax.axis('tight'); ax.axis('off')\n",
    "table = ax.table(cellText=df.values, colLabels=df.columns, cellLoc='center', loc='center')\n",
    "table.auto_set_font_size(False); table.set_fontsize(9); table.scale(1.2, 1.5)\n",
    "\n",
    "# \n",
    "for i in range(len(df.columns)):\n",
    "    table[(0, i)].set_facecolor('#4CAF50'); table[(0, i)].set_text_props(weight='bold', color='white')\n",
    "# \n",
    "for i in range(1, len(df)):\n",
    "    for j in range(len(df.columns)):\n",
    "        if i % 2 == 0:\n",
    "            table[(i, j)].set_facecolor('#F5F5F5')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(OUT_PNG, bbox_inches=\"tight\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "print(f\" \\n  - {OUT_CSV}\\n  - {OUT_PNG}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "830e6943-e7da-42eb-9207-6c6f84633315",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: Radar chart (normalized multi-metric) ============================\n",
    "import numpy as np, pandas as pd, matplotlib.pyplot as plt, os, math\n",
    "import re #  re \n",
    "\n",
    "# ====================================================================\n",
    "# \n",
    "# ====================================================================\n",
    "\n",
    "# \n",
    "PLOT_DPI = 300\n",
    "FIGURE_SIZE = (8, 8)\n",
    "FONT_SIZE_METRIC = 15\n",
    "FONT_SIZE_RADIAL = 12\n",
    "# \n",
    "FONT_SIZE_LEGEND = 12 # <--- \n",
    "\n",
    "# /\n",
    "MODEL_LINEWIDTH = 2.5 # \n",
    "SPINE_LINEWIDTH = 2.5 # \n",
    "\n",
    "# \n",
    "LEGEND_LOC = \"upper right\"\n",
    "LEGEND_BBOX_TO_ANCHOR = (1.25, 1.10) \n",
    "LEGEND_RIGHT_ADJUST_FACTOR = 0.05 \n",
    "\n",
    "#  (0.2, 0.4 )\n",
    "RADIAL_TICKS = [0.2, 0.4, 0.6, 0.8]\n",
    "R_LABEL_POSITION_DEG = 22.5 \n",
    "R_LABEL_PAD = 10\n",
    "\n",
    "#  ()\n",
    "PLOT_TITLE = \"Radar (normalized)\"\n",
    "TITLE_PAD = 20\n",
    "\n",
    "#  (WAPE)\n",
    "ROTATION_THRESHOLD_DEG = 5 \n",
    "\n",
    "# ====================================================================\n",
    "# --- 1.  () ---\n",
    "# ... () ...\n",
    "# ====================================================================\n",
    "# ****\n",
    "ALIASES_LOWER = {\n",
    "    \"tcn\": \"TCN\",\n",
    "    \"rnn\": \"RNN\", \"rnn_tanh\": \"RNN\", \"rnn-tanh\": \"RNN\", \"rnn tanh\": \"RNN\", \"vanilla rnn\": \"RNN\",\n",
    "    \"lstm\": \"LSTM\",\n",
    "    \"bilstm\": \"BiLSTM\", \"bi-lstm\": \"BiLSTM\", \"bi lstm\": \"BiLSTM\", \"bi_lstm\": \"BiLSTM\",\n",
    "    \"cnn_lstm\": \"CNN_LSTM\", \"cnn-lstm\": \"CNN_LSTM\", \"cnn lstm\": \"CNN_LSTM\",\n",
    "    \"seq2seq_lstm\": \"Seq2Seq_LSTM\", \"seq2seq-lstm\": \"Seq2Seq_LSTM\", \"seq2seq lstm\": \"Seq2Seq_LSTM\",\n",
    "    \"gru\": \"GRU\",\n",
    "    \"transformer\": \"Transformer\",\n",
    "    \"resmlp\": \"ResMLP\",\n",
    "}\n",
    "\n",
    "def to_canonical(name: str) -> str:\n",
    "    \"\"\"_- RNN_tanh  RNN\"\"\"\n",
    "    if not isinstance(name, str):\n",
    "        return str(name)\n",
    "    s = name.strip().lower()\n",
    "    s = re.sub(r\"[^\\w\\s\\-]+\", \"\", s)         # \n",
    "    s = s.replace(\"__\",\"_\").replace(\"-\", \" \") # \n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    \n",
    "    # \n",
    "    if s in ALIASES_LOWER:\n",
    "        return ALIASES_LOWER[s]\n",
    "    # \n",
    "    s2 = s.replace(\" \", \"_\")\n",
    "    if s2 in ALIASES_LOWER:\n",
    "        return ALIASES_LOWER[s2]\n",
    "    # \n",
    "    return name.strip()\n",
    "\n",
    "\n",
    "#  VIZ_DIR, METRICS_ORDER, MIN_BETTER, TCN_NAME \n",
    "\n",
    "avg = pd.read_csv(os.path.join(VIZ_DIR, \"all_models_averages.csv\"))\n",
    "\n",
    "# ** to_canonical **\n",
    "avg[\"Model\"] = avg[\"Model\"].apply(to_canonical) \n",
    "\n",
    "avg = avg[[\"Model\"] + METRICS_ORDER].copy()\n",
    "\n",
    "# 1. \n",
    "REQUIRED_ORDER = [\n",
    "    \"TCN\", \"RNN\", \"LSTM\", \"BiLSTM\", \"CNN_LSTM\", \n",
    "    \"Seq2Seq_LSTM\", \"GRU\", \"Transformer\", \"ResMLP\"\n",
    "]\n",
    "\n",
    "# None =  MSE  3 + TCN\n",
    "SELECTED_MODELS_AUTO = None \n",
    "\n",
    "if SELECTED_MODELS_AUTO is None:\n",
    "    tmp = avg[[\"Model\",\"MSE\"]].dropna().sort_values(\"MSE\")\n",
    "    auto = list(tmp[\"Model\"].values[:10])\n",
    "    \n",
    "    #  avg  REQUIRED_ORDER \n",
    "    all_models_in_avg = set(avg[\"Model\"].values)\n",
    "    \n",
    "    SELECTED_MODELS = [m for m in REQUIRED_ORDER if m in all_models_in_avg]\n",
    "    for m in auto:\n",
    "        if m not in SELECTED_MODELS and m in all_models_in_avg:\n",
    "            SELECTED_MODELS.append(m)\n",
    "            \n",
    "    if not SELECTED_MODELS:\n",
    "        SELECTED_MODELS = list(tmp[\"Model\"].values[:3])\n",
    "\n",
    "    print(\"\", SELECTED_MODELS)\n",
    "else:\n",
    "    #  to_canonical\n",
    "    SELECTED_MODELS = [to_canonical(m) for m in SELECTED_MODELS_AUTO]\n",
    "\n",
    "#  SELECTED_MODELS \n",
    "sub = avg[avg[\"Model\"].isin(SELECTED_MODELS)].set_index(\"Model\")\n",
    "sub = sub.reindex(SELECTED_MODELS) # \n",
    "\n",
    "#  =>  1 =>  1\n",
    "def normalize_series(vals, minimize=True):\n",
    "    v = np.array(vals, dtype=float)\n",
    "    if np.all(np.isnan(v)): return np.zeros_like(v) + 0.5\n",
    "    mn, mx = np.nanmin(v), np.nanmax(v)\n",
    "    if not math.isfinite(mn) or not math.isfinite(mx) or mn==mx:\n",
    "        return np.zeros_like(v) + 0.5\n",
    "    if minimize:\n",
    "        return 1.0 - (v - mn) / (mx - mn)\n",
    "    else:\n",
    "        return (v - mn) / (mx - mn)\n",
    "\n",
    "normed = {}\n",
    "for met in METRICS_ORDER:\n",
    "    minimize = met in MIN_BETTER \n",
    "    normed[met] = normalize_series(sub[met].values, minimize=minimize)\n",
    "\n",
    "# ====================================================================\n",
    "# --- 2.  () ---\n",
    "# ====================================================================\n",
    "\n",
    "#  METRICS_ORDER\n",
    "metrics_count = len(METRICS_ORDER)\n",
    "angles = np.linspace(0, 2*np.pi, metrics_count, endpoint=False).tolist()\n",
    "angles += angles[:1]  # \n",
    "fig, ax = plt.subplots(figsize=FIGURE_SIZE, subplot_kw=dict(polar=True), dpi=160) \n",
    "\n",
    "# \n",
    "for i, (model, row) in enumerate(sub.iterrows()):\n",
    "    vals = [normed[met][i] for met in METRICS_ORDER] \n",
    "    vals += vals[:1]\n",
    "    ax.plot(angles, vals, label=model, linewidth=MODEL_LINEWIDTH, marker='o') \n",
    "    ax.fill(angles, vals, alpha=0.1)\n",
    "\n",
    "# \n",
    "for spine in ax.spines.values():\n",
    "    spine.set_linewidth(SPINE_LINEWIDTH)\n",
    "    spine.set_color('black') \n",
    "\n",
    "#  ()\n",
    "ax.set_xticks(angles[:-1])\n",
    "\n",
    "# ****\n",
    "#  X \n",
    "labels = ax.set_xticklabels(METRICS_ORDER, fontsize=FONT_SIZE_METRIC)\n",
    "\n",
    "# \n",
    "for label, angle in zip(labels, angles[:-1]):\n",
    "    # \n",
    "    deg = angle * 180 / np.pi\n",
    "    \n",
    "    # \n",
    "    if deg > 90 and deg < 270:\n",
    "        # \n",
    "        rotation = deg + 90\n",
    "        align = 'right'\n",
    "    else:\n",
    "        # \n",
    "        rotation = deg - 90\n",
    "        align = 'left'\n",
    "\n",
    "    # 0, 180\n",
    "    if deg < ROTATION_THRESHOLD_DEG or deg > (360 - ROTATION_THRESHOLD_DEG): #  (0)\n",
    "        rotation = 0\n",
    "        align = 'left'\n",
    "    elif deg > (180 - ROTATION_THRESHOLD_DEG) and deg < (180 + ROTATION_THRESHOLD_DEG): #  (180)\n",
    "        rotation = 0\n",
    "        align = 'right'\n",
    "        \n",
    "    label.set_rotation(rotation)\n",
    "    label.set_horizontalalignment(align)\n",
    "    label.set_verticalalignment('bottom') \n",
    "    \n",
    "# 0.2, 0.4\n",
    "ax.set_yticks(RADIAL_TICKS)\n",
    "ax.set_yticklabels([str(t) for t in RADIAL_TICKS], fontsize=FONT_SIZE_RADIAL)\n",
    "ax.set_rlabel_position(R_LABEL_POSITION_DEG) \n",
    "ax.tick_params(axis='y', pad=R_LABEL_PAD)\n",
    "\n",
    "# #  ()\n",
    "# if PLOT_TITLE:\n",
    "#     ax.set_title(PLOT_TITLE, pad=TITLE_PAD)\n",
    "    \n",
    "# \n",
    "ax.legend(loc=LEGEND_LOC, bbox_to_anchor=LEGEND_BBOX_TO_ANCHOR, fontsize=FONT_SIZE_LEGEND)\n",
    "\n",
    "plt.tight_layout()\n",
    "# ** LEGEND_RIGHT_ADJUST_FACTOR **\n",
    "plt.subplots_adjust(right=LEGEND_BBOX_TO_ANCHOR[0] + LEGEND_RIGHT_ADJUST_FACTOR) \n",
    "\n",
    "plt.savefig(os.path.join(VIZ_DIR, \"radar_normalized.png\"), bbox_inches=\"tight\", dpi=PLOT_DPI)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca847808-da52-432a-a4f1-9715714073d5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
